{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f974640",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install portaudio via homebrew: brew install portaudio\n",
    "%pip install azure-ai-voicelive azure-identity azure-search-documents python-dotenv pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120a153",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Union, Optional, TYPE_CHECKING, cast, List, Dict, Any, Callable, Awaitable\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials_async import AsyncTokenCredential\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "from azure.ai.voicelive.aio import connect\n",
    "from azure.ai.voicelive.models import (\n",
    "    AudioEchoCancellation,\n",
    "    AudioInputTranscriptionOptions,\n",
    "    AudioNoiseReduction,\n",
    "    AzureSemanticVad,\n",
    "    AzureStandardVoice,\n",
    "    InputAudioFormat,\n",
    "    Modality,\n",
    "    OutputAudioFormat,\n",
    "    RequestSession,\n",
    "    ServerEventType,\n",
    "    ServerVad,\n",
    "    FunctionCallOutputItem,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pyaudio\n",
    "\n",
    "# Import reusable modules from local files\n",
    "from audio_processor import AudioProcessor\n",
    "\n",
    "# Import RAG tools - both AI Search and Redis backends\n",
    "from rag_tools import RAGToolManager, create_rag_tools_from_env, ToolResult, ToolResultDirection\n",
    "from rag_tools_redis import RedisRAGToolManager, create_redis_rag_tools_from_env\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from azure.ai.voicelive.aio import VoiceLiveConnection\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./.env', override=True)\n",
    "\n",
    "# Setup logging\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.basicConfig(\n",
    "    filename=f'logs/{timestamp}_rag_assistant.log',\n",
    "    filemode=\"w\",\n",
    "    format='%(asctime)s:%(name)s:%(levelname)s:%(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"âœ… Libraries imported and logging configured\")\n",
    "print(\"   Available RAG backends: 'ai_search', 'redis'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f508e",
   "metadata": {},
   "source": [
    "## 3. RAG Voice Assistant Class\n",
    "\n",
    "This class extends the basic voice assistant with tool/function calling capabilities for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGVoiceAssistant:\n",
    "    \"\"\"\n",
    "    Voice assistant with RAG (Retrieval-Augmented Generation) capabilities.\n",
    "    \n",
    "    Extends the basic voice assistant to support:\n",
    "    - Tool/function calling for knowledge base search\n",
    "    - Source citation and grounding\n",
    "    - Multi-turn conversations with context\n",
    "    - Multiple RAG backends: Azure AI Search or Redis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        credential: Union[AzureKeyCredential, AsyncTokenCredential],\n",
    "        model: str,\n",
    "        voice: str,\n",
    "        instructions: str,\n",
    "        rag_tools: Union[RAGToolManager, RedisRAGToolManager],\n",
    "        voice_temperature: Optional[float] = None,\n",
    "        input_audio_transcription_enabled: bool = False,\n",
    "        input_audio_transcription_model: Optional[str] = None,\n",
    "        vad_config: Optional[Dict] = None,\n",
    "    ):\n",
    "        self.endpoint = endpoint\n",
    "        self.credential = credential\n",
    "        self.model = model\n",
    "        self.voice = voice\n",
    "        self.instructions = instructions\n",
    "        self.rag_tools = rag_tools\n",
    "        self.voice_temperature = voice_temperature\n",
    "        self.input_audio_transcription_enabled = input_audio_transcription_enabled\n",
    "        self.input_audio_transcription_model = input_audio_transcription_model\n",
    "        self.vad_config = vad_config or {}\n",
    "        \n",
    "        # Auto-detect language from voice\n",
    "        if voice and \"-\" in voice:\n",
    "            parts = voice.split(\"-\")\n",
    "            if len(parts) >= 2:\n",
    "                self.transcription_language = f\"{parts[0]}-{parts[1].split(':')[0]}\"\n",
    "            else:\n",
    "                self.transcription_language = \"en-US\"\n",
    "        else:\n",
    "            self.transcription_language = \"en-US\"\n",
    "        \n",
    "        self.connection: Optional[\"VoiceLiveConnection\"] = None\n",
    "        self.audio_processor: Optional[AudioProcessor] = None\n",
    "        self.session_ready = False\n",
    "        self._active_response = False\n",
    "        self._response_api_done = False\n",
    "        self._stop_requested = False\n",
    "        self._assistant_transcript_buffer = \"\"\n",
    "        \n",
    "        # Tool call tracking\n",
    "        self._pending_tool_calls: Dict[str, Dict[str, Any]] = {}\n",
    "        self._tool_call_arguments: Dict[str, str] = {}\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Request graceful stop.\"\"\"\n",
    "        self._stop_requested = True\n",
    "        print(\"\\nğŸ›‘ Stop requested, shutting down...\")\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the voice assistant session.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Connecting to VoiceLive API with model %s\", self.model)\n",
    "\n",
    "            async with connect(\n",
    "                endpoint=self.endpoint,\n",
    "                credential=self.credential,\n",
    "                model=self.model,\n",
    "            ) as connection:\n",
    "                conn = connection\n",
    "                self.connection = conn\n",
    "\n",
    "                ap = AudioProcessor(conn)\n",
    "                self.audio_processor = ap\n",
    "\n",
    "                await self._setup_session()\n",
    "                ap.start_playback()\n",
    "\n",
    "                logger.info(\"RAG Voice assistant ready!\")\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"ğŸ¤ RAG VOICE ASSISTANT READY\")\n",
    "                print(\"Ask questions about your knowledge base!\")\n",
    "                print(\"The assistant will search and cite sources.\")\n",
    "                print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "                await self._process_events()\n",
    "        finally:\n",
    "            if self.audio_processor:\n",
    "                self.audio_processor.shutdown()\n",
    "            if self.rag_tools:\n",
    "                await self.rag_tools.close()\n",
    "\n",
    "    async def _setup_session(self):\n",
    "        \"\"\"Configure the VoiceLive session with RAG tools.\"\"\"\n",
    "        logger.info(\"Setting up RAG voice session...\")\n",
    "\n",
    "        # Configure voice\n",
    "        voice_config: Union[AzureStandardVoice, str]\n",
    "        if \"-\" in self.voice:\n",
    "            voice_config = AzureStandardVoice(\n",
    "                name=self.voice,\n",
    "                temperature=self.voice_temperature\n",
    "            )\n",
    "        else:\n",
    "            voice_config = self.voice\n",
    "\n",
    "        # VAD configuration\n",
    "        vad_type = self.vad_config.get(\"type\", \"azure_semantic_vad\")\n",
    "        if vad_type == \"azure_semantic_vad\":\n",
    "            turn_detection_config = AzureSemanticVad(\n",
    "                threshold=self.vad_config.get(\"threshold\", 0.5),\n",
    "                prefix_padding_ms=self.vad_config.get(\"prefix_padding_ms\", 300),\n",
    "                speech_duration_ms=self.vad_config.get(\"speech_duration_ms\", 80),\n",
    "                silence_duration_ms=self.vad_config.get(\"silence_duration_ms\", 500),\n",
    "            )\n",
    "        else:\n",
    "            turn_detection_config = ServerVad(\n",
    "                threshold=self.vad_config.get(\"threshold\", 0.8),\n",
    "                prefix_padding_ms=self.vad_config.get(\"prefix_padding_ms\", 200),\n",
    "                silence_duration_ms=self.vad_config.get(\"silence_duration_ms\", 1000),\n",
    "            )\n",
    "\n",
    "        # Input transcription\n",
    "        input_transcription_config = None\n",
    "        if self.input_audio_transcription_enabled:\n",
    "            input_transcription_config = AudioInputTranscriptionOptions(\n",
    "                model=self.input_audio_transcription_model or \"azure-speech\",\n",
    "                language=self.transcription_language,\n",
    "            )\n",
    "\n",
    "        # Get tool schemas from RAG manager\n",
    "        tools = self.rag_tools.get_tool_schemas()\n",
    "        logger.info(f\"Configured {len(tools)} RAG tools\")\n",
    "\n",
    "        session_config = RequestSession(\n",
    "            modalities=[Modality.TEXT, Modality.AUDIO],\n",
    "            instructions=self.instructions,\n",
    "            voice=voice_config,\n",
    "            input_audio_format=InputAudioFormat.PCM16,\n",
    "            output_audio_format=OutputAudioFormat.PCM16,\n",
    "            turn_detection=turn_detection_config,\n",
    "            input_audio_echo_cancellation=AudioEchoCancellation(),\n",
    "            input_audio_noise_reduction=AudioNoiseReduction(type=\"azure_deep_noise_suppression\"),\n",
    "            input_audio_transcription=input_transcription_config,\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        conn = self.connection\n",
    "        assert conn is not None\n",
    "        await conn.session.update(session=session_config)\n",
    "        logger.info(\"Session configuration sent with RAG tools\")\n",
    "\n",
    "    async def _process_events(self):\n",
    "        \"\"\"Process events from VoiceLive.\"\"\"\n",
    "        try:\n",
    "            conn = self.connection\n",
    "            assert conn is not None\n",
    "            async for event in conn:\n",
    "                if self._stop_requested:\n",
    "                    break\n",
    "                await self._handle_event(event)\n",
    "        except asyncio.CancelledError:\n",
    "            logger.info(\"Event processing cancelled\")\n",
    "            raise\n",
    "        except Exception:\n",
    "            logger.exception(\"Error processing events\")\n",
    "            raise\n",
    "\n",
    "    async def _handle_event(self, event):\n",
    "        \"\"\"Handle different types of events from VoiceLive.\"\"\"\n",
    "        ap = self.audio_processor\n",
    "        conn = self.connection\n",
    "        assert ap is not None and conn is not None\n",
    "\n",
    "        if event.type == ServerEventType.SESSION_UPDATED:\n",
    "            logger.info(\"Session ready: %s\", event.session.id)\n",
    "            self.session_ready = True\n",
    "            ap.start_capture()\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:\n",
    "            logger.info(\"User started speaking\")\n",
    "            print(\"ğŸ¤ Listening...\")\n",
    "            ap.skip_pending_audio()\n",
    "            if self._active_response and not self._response_api_done:\n",
    "                try:\n",
    "                    await conn.response.cancel()\n",
    "                except Exception as e:\n",
    "                    if \"no active response\" not in str(e).lower():\n",
    "                        logger.warning(\"Cancel failed: %s\", e)\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED:\n",
    "            print(\"ğŸ¤” Processing...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_CREATED:\n",
    "            self._active_response = True\n",
    "            self._response_api_done = False\n",
    "            self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:\n",
    "            ap.queue_audio(event.delta)\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DONE:\n",
    "            print(\"ğŸ¤ Ready for next input...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_DONE:\n",
    "            self._active_response = False\n",
    "            self._response_api_done = True\n",
    "            \n",
    "            # Check if there are pending tool calls to process\n",
    "            if self._pending_tool_calls:\n",
    "                await self._process_pending_tool_calls()\n",
    "\n",
    "        # Handle function/tool calls\n",
    "        elif event.type == ServerEventType.RESPONSE_OUTPUT_ITEM_ADDED:\n",
    "            if hasattr(event, 'item') and hasattr(event.item, 'type'):\n",
    "                if event.item.type == \"function_call\":\n",
    "                    item = event.item\n",
    "                    call_id = getattr(item, 'call_id', None)\n",
    "                    if call_id:\n",
    "                        self._pending_tool_calls[call_id] = {\n",
    "                            \"name\": getattr(item, 'name', None),\n",
    "                            \"item_id\": getattr(item, 'id', None),\n",
    "                        }\n",
    "                        self._tool_call_arguments[call_id] = \"\"\n",
    "                        logger.info(f\"Tool call started: {item.name} ({call_id})\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA:\n",
    "            call_id = getattr(event, 'call_id', None)\n",
    "            delta = getattr(event, 'delta', '')\n",
    "            if call_id and call_id in self._tool_call_arguments:\n",
    "                self._tool_call_arguments[call_id] += delta\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE:\n",
    "            call_id = getattr(event, 'call_id', None)\n",
    "            if call_id and call_id in self._pending_tool_calls:\n",
    "                self._pending_tool_calls[call_id][\"arguments\"] = self._tool_call_arguments.get(call_id, \"{}\")\n",
    "                logger.info(f\"Tool call arguments complete: {call_id}\")\n",
    "\n",
    "        # Transcription events\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DELTA:\n",
    "            delta = getattr(event, 'delta', None)\n",
    "            if delta:\n",
    "                self._assistant_transcript_buffer += delta\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DONE:\n",
    "            transcript = getattr(event, 'transcript', None) or self._assistant_transcript_buffer\n",
    "            if transcript:\n",
    "                print(f\"ğŸ¤– Assistant: {transcript}\")\n",
    "            self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:\n",
    "            transcript = getattr(event, 'transcript', None)\n",
    "            if transcript:\n",
    "                print(f\"ğŸ“ You said: {transcript}\")\n",
    "\n",
    "        elif event.type == ServerEventType.ERROR:\n",
    "            msg = event.error.message\n",
    "            if \"no active response\" not in msg.lower():\n",
    "                print(f\"âŒ Error: {msg}\")\n",
    "                logger.error(\"VoiceLive error: %s\", msg)\n",
    "\n",
    "    async def _process_pending_tool_calls(self):\n",
    "        \"\"\"Process all pending tool calls.\"\"\"\n",
    "        conn = self.connection\n",
    "        assert conn is not None\n",
    "        \n",
    "        tool_handlers = self.rag_tools.get_tool_handlers()\n",
    "        \n",
    "        for call_id, call_info in list(self._pending_tool_calls.items()):\n",
    "            tool_name = call_info.get(\"name\")\n",
    "            arguments_str = call_info.get(\"arguments\", \"{}\")\n",
    "            \n",
    "            if not tool_name:\n",
    "                continue\n",
    "            \n",
    "            handler = tool_handlers.get(tool_name)\n",
    "            if not handler:\n",
    "                logger.warning(f\"No handler for tool: {tool_name}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                args = json.loads(arguments_str)\n",
    "            except json.JSONDecodeError:\n",
    "                args = {}\n",
    "            \n",
    "            print(f\"ğŸ”§ Calling tool: {tool_name}\")\n",
    "            logger.info(f\"Executing tool: {tool_name} with args: {args}\")\n",
    "            \n",
    "            try:\n",
    "                result = await handler(args)\n",
    "                result_text = result.to_text()\n",
    "                \n",
    "                # Send tool result back to VoiceLive\n",
    "                output_item = FunctionCallOutputItem(\n",
    "                    call_id=call_id,\n",
    "                    output=result_text,\n",
    "                )\n",
    "                await conn.conversation.item.create(item=output_item)\n",
    "                await conn.response.create()\n",
    "                \n",
    "                logger.info(f\"Tool result sent for {tool_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Tool execution failed: {tool_name}\")\n",
    "                # Send error result\n",
    "                error_output = FunctionCallOutputItem(\n",
    "                    call_id=call_id,\n",
    "                    output=json.dumps({\"error\": str(e)}),\n",
    "                )\n",
    "                await conn.conversation.item.create(item=error_output)\n",
    "                await conn.response.create()\n",
    "        \n",
    "        # Clear processed calls\n",
    "        self._pending_tool_calls.clear()\n",
    "        self._tool_call_arguments.clear()\n",
    "\n",
    "\n",
    "print(\"âœ… RAGVoiceAssistant class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e6c35",
   "metadata": {},
   "source": [
    "## 4. Check Audio System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2efddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_system():\n",
    "    \"\"\"Check if audio input/output devices are available.\"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        input_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxInputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        output_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxOutputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        p.terminate()\n",
    "\n",
    "        if not input_devices:\n",
    "            print(\"âŒ No audio input devices found.\")\n",
    "            return False\n",
    "        if not output_devices:\n",
    "            print(\"âŒ No audio output devices found.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"âœ… Found {len(input_devices)} input device(s) and {len(output_devices)} output device(s)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Audio system check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "audio_ok = check_audio_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069952",
   "metadata": {},
   "source": [
    "## 5. Run Voice Assistant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_rag_voice_assistant():\n",
    "    \"\"\"Run the RAG voice assistant.\"\"\"\n",
    "    # Validate credentials\n",
    "    if not API_KEY and not USE_TOKEN_CREDENTIAL:\n",
    "        print(\"âŒ Error: No authentication provided\")\n",
    "        return\n",
    "\n",
    "    # Create credential\n",
    "    credential: Union[AzureKeyCredential, AsyncTokenCredential]\n",
    "    if USE_TOKEN_CREDENTIAL:\n",
    "        credential = AzureCliCredential()\n",
    "        print(\"ğŸ” Using Azure CLI credential\")\n",
    "    else:\n",
    "        credential = AzureKeyCredential(API_KEY)\n",
    "        print(\"ğŸ”‘ Using API key credential\")\n",
    "\n",
    "    # Create RAG tools based on selected backend\n",
    "    rag_tools: Union[RAGToolManager, RedisRAGToolManager]\n",
    "    \n",
    "    if RAG_BACKEND == \"redis\":\n",
    "        # Use Redis backend\n",
    "        rag_tools = create_redis_rag_tools_from_env()\n",
    "        backend_name = \"Azure Managed Redis\"\n",
    "        if rag_tools.redis_client:\n",
    "            print(f\"âœ… RAG tools initialized with Redis index: {REDIS_INDEX_NAME}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Redis RAG not configured - assistant will work without knowledge base\")\n",
    "    else:\n",
    "        # Use Azure AI Search backend (default)\n",
    "        rag_tools = create_rag_tools_from_env()\n",
    "        backend_name = \"Azure AI Search\"\n",
    "        if rag_tools.search_client:\n",
    "            print(f\"âœ… RAG tools initialized with AI Search index: {SEARCH_INDEX}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ AI Search RAG not configured - assistant will work without knowledge base\")\n",
    "\n",
    "    # Create and start voice assistant\n",
    "    assistant = RAGVoiceAssistant(\n",
    "        endpoint=ENDPOINT,\n",
    "        credential=credential,\n",
    "        model=MODEL,\n",
    "        voice=VOICE,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        rag_tools=rag_tools,\n",
    "        voice_temperature=VOICE_TEMPERATURE,\n",
    "        input_audio_transcription_enabled=INPUT_AUDIO_TRANSCRIPTION_ENABLED,\n",
    "        input_audio_transcription_model=INPUT_AUDIO_TRANSCRIPTION_MODEL,\n",
    "        vad_config=VAD_CONFIG,\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ™ï¸  RAG Voice Assistant with Azure VoiceLive SDK\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ—„ï¸  RAG Backend: {backend_name}\")\n",
    "    if RAG_BACKEND == \"redis\":\n",
    "        print(f\"ğŸ“š Knowledge base: {REDIS_INDEX_NAME or 'Not configured'}\")\n",
    "    else:\n",
    "        print(f\"ğŸ“š Knowledge base: {SEARCH_INDEX or 'Not configured'}\")\n",
    "        print(f\"ğŸ” Vector search: {'Enabled' if USE_VECTOR_SEARCH else 'Disabled'}\")\n",
    "\n",
    "    try:\n",
    "        await assistant.start()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ‘‹ Voice assistant shut down. Goodbye!\")\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nğŸ‘‹ Voice assistant interrupted. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fatal Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"âœ… run_rag_voice_assistant() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92866b1",
   "metadata": {},
   "source": [
    "## 6. Configuration & Run\n",
    "\n",
    "Update the settings below and run this cell to start the RAG voice assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VOICELIVE CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "API_KEY = os.environ.get(\"AZURE_VOICELIVE_API_KEY\")\n",
    "ENDPOINT = os.environ.get(\"AZURE_VOICELIVE_ENDPOINT\",\n",
    "                          \"https://your-resource.services.ai.azure.com/\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Voice Configuration\n",
    "VOICE = \"zh-CN-Xiaochen:DragonHDLatestNeural\"\n",
    "VOICE_TEMPERATURE = 0.8\n",
    "\n",
    "# Transcription\n",
    "INPUT_AUDIO_TRANSCRIPTION_ENABLED = True\n",
    "INPUT_AUDIO_TRANSCRIPTION_MODEL = \"azure-speech\"\n",
    "\n",
    "# VAD Configuration\n",
    "VAD_CONFIG: Dict = {\n",
    "    \"type\": \"azure_semantic_vad\",\n",
    "    \"threshold\": 0.3,\n",
    "    \"prefix_padding_ms\": 300,\n",
    "    \"silence_duration_ms\": 500,\n",
    "    \"speech_duration_ms\": 80,\n",
    "}\n",
    "\n",
    "# Authentication\n",
    "USE_TOKEN_CREDENTIAL = False\n",
    "\n",
    "# ============================================================\n",
    "# RAG BACKEND SELECTION\n",
    "# ============================================================\n",
    "# Choose your RAG backend: \"ai_search\" or \"redis\"\n",
    "\n",
    "RAG_BACKEND = \"redis\"  # Options: \"ai_search\", \"redis\"\n",
    "\n",
    "# ============================================================\n",
    "# AZURE AI SEARCH CONFIGURATION (when RAG_BACKEND = \"ai_search\")\n",
    "# ============================================================\n",
    "\n",
    "SEARCH_ENDPOINT = os.environ.get(\"AZURE_SEARCH_ENDPOINT\")\n",
    "SEARCH_INDEX = os.environ.get(\"AZURE_SEARCH_INDEX\")\n",
    "SEARCH_API_KEY = os.environ.get(\"AZURE_SEARCH_API_KEY\")\n",
    "\n",
    "# AI Search specific configuration\n",
    "SEMANTIC_CONFIGURATION = os.environ.get(\"AZURE_SEARCH_SEMANTIC_CONFIGURATION\", \"default\")\n",
    "IDENTIFIER_FIELD = os.environ.get(\"AZURE_SEARCH_IDENTIFIER_FIELD\", \"id\")\n",
    "CONTENT_FIELD = os.environ.get(\"AZURE_SEARCH_CONTENT_FIELD\", \"content\")\n",
    "EMBEDDING_FIELD = os.environ.get(\"AZURE_SEARCH_EMBEDDING_FIELD\", \"embedding\")\n",
    "TITLE_FIELD = os.environ.get(\"AZURE_SEARCH_TITLE_FIELD\")\n",
    "USE_VECTOR_SEARCH = os.environ.get(\"AZURE_SEARCH_USE_VECTOR\", \"true\").lower() == \"true\"\n",
    "\n",
    "# ============================================================\n",
    "# AZURE MANAGED REDIS CONFIGURATION (when RAG_BACKEND = \"redis\")\n",
    "# ============================================================\n",
    "\n",
    "REDIS_HOST = os.environ.get(\"AZURE_REDIS_HOST\", \"xle-redis2.eastus.redis.azure.net\")\n",
    "REDIS_PORT = int(os.environ.get(\"AZURE_REDIS_PORT\", \"10000\"))\n",
    "REDIS_PASSWORD = os.environ.get(\"AZURE_REDIS_PASSWORD\")\n",
    "REDIS_INDEX_NAME = os.environ.get(\"AZURE_REDIS_INDEX_NAME\", \"product_index\")\n",
    "\n",
    "# ============================================================\n",
    "# SYSTEM INSTRUCTIONS (Optimized for Speech)\n",
    "# ============================================================\n",
    "\n",
    "INSTRUCTIONS = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è¯­éŸ³åŠ©æ‰‹ï¼Œå¯ä»¥è®¿é—®çŸ¥è¯†åº“æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "\n",
    "è¯­éŸ³å¯¹è¯è§„èŒƒï¼š\n",
    "- ä½¿ç”¨è‡ªç„¶ã€å£è¯­åŒ–çš„ä¸­æ–‡è¡¨è¾¾ï¼Œå°±åƒåœ¨å’Œæœ‹å‹èŠå¤©ä¸€æ ·\n",
    "- ä¸è¦ä½¿ç”¨é¡¹ç›®ç¬¦å·ã€ç¼–å·åˆ—è¡¨æˆ–ç‰¹æ®Šæ ¼å¼ç¬¦å·\n",
    "- ç”¨ç®€çŸ­çš„å¥å­ï¼Œé€‚å½“åœé¡¿ï¼Œè®©å¬ä¼—å®¹æ˜“ç†è§£\n",
    "- ä½¿ç”¨è¿‡æ¸¡è¯è®©å›ç­”æ›´æµç•…ï¼Œæ¯”å¦‚\"é¦–å…ˆ\"ã€\"å¦å¤–\"ã€\"æ€»çš„æ¥è¯´\"\n",
    "\n",
    "æ•°å­—å’Œå•ä½çš„å¤„ç†ï¼š\n",
    "- ä¿æŒæ•°å­—çš„é˜¿æ‹‰ä¼¯æ•°å­—å½¢å¼ï¼Œå¦‚ 6.78 è‹±å¯¸ã€2448x1080ã€165Hz\n",
    "- è¯­éŸ³åˆæˆç³»ç»Ÿä¼šè‡ªåŠ¨å°†æ•°å­—è½¬æ¢ä¸ºè‡ªç„¶çš„è¯­éŸ³\n",
    "- ä¸è¦æ‰‹åŠ¨å°†æ•°å­—è½¬å†™æˆä¸­æ–‡ï¼Œå¦‚ä¸è¦å†™\"å…­ç‚¹ä¸ƒå…«è‹±å¯¸\"\n",
    "\n",
    "å›ç­”é—®é¢˜æ—¶ï¼š\n",
    "- å…ˆä½¿ç”¨ internal_search å·¥å…·åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯\n",
    "- æ ¹æ®æœç´¢ç»“æœç®€æ´åœ°å›ç­”é—®é¢˜\n",
    "- ä½¿ç”¨ report_grounding å·¥å…·è®°å½•å¼•ç”¨çš„æ¥æº\n",
    "- å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå‹å¥½åœ°å‘Šè¯‰ç”¨æˆ·ï¼Œå¹¶è¯¢é—®æ˜¯å¦éœ€è¦å…¶ä»–å¸®åŠ©\n",
    "\n",
    "ä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œåƒæ­£å¸¸å¯¹è¯ä¸€æ ·è‡ªç„¶ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# STATUS DISPLAY\n",
    "# ============================================================\n",
    "\n",
    "print(\"ğŸ“ VoiceLive Configuration:\")\n",
    "print(f\"   Endpoint: {ENDPOINT}\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   Voice: {VOICE}\")\n",
    "print(f\"   API Key: {'Set' if API_KEY else 'Not set'}\")\n",
    "print()\n",
    "print(f\"ğŸ—„ï¸  RAG Backend: {RAG_BACKEND.upper()}\")\n",
    "print()\n",
    "\n",
    "if RAG_BACKEND == \"redis\":\n",
    "    print(\"ğŸ”´ Azure Managed Redis Configuration:\")\n",
    "    print(f\"   Host: {REDIS_HOST}:{REDIS_PORT}\")\n",
    "    print(f\"   Password: {'Set' if REDIS_PASSWORD else 'Not set'}\")\n",
    "    print(f\"   Index: {REDIS_INDEX_NAME}\")\n",
    "else:\n",
    "    print(\"ğŸ” Azure AI Search Configuration:\")\n",
    "    print(f\"   Endpoint: {SEARCH_ENDPOINT or 'Not set'}\")\n",
    "    print(f\"   Index: {SEARCH_INDEX or 'Not set'}\")\n",
    "    print(f\"   API Key: {'Set' if SEARCH_API_KEY else 'Not set'}\")\n",
    "    print(f\"   Vector Search: {'Enabled' if USE_VECTOR_SEARCH else 'Disabled'}\")\n",
    "    print(f\"   Semantic Config: {SEMANTIC_CONFIGURATION}\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN THE VOICE ASSISTANT\n",
    "# ============================================================\n",
    "\n",
    "if audio_ok:\n",
    "    try:\n",
    "        await run_rag_voice_assistant()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nğŸ‘‹ Session ended. Goodbye!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Please fix audio issues before running the voice assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f6634",
   "metadata": {},
   "source": [
    "## 7. View Cited Sources\n",
    "\n",
    "After a conversation, you can view the sources that were cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14706295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be run after the conversation to see cited sources\n",
    "# Note: You'll need to keep a reference to the rag_tools object\n",
    "\n",
    "# Example:\n",
    "# sources = rag_tools.get_cited_sources()\n",
    "# print(f\"Sources cited during conversation: {sources}\")\n",
    "\n",
    "print(\"Run the voice assistant first, then check cited sources here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f1586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
