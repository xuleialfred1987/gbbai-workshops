{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b051edb",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install portaudio via homebrew: brew install portaudio\n",
    "# Then install Python packages\n",
    "%pip install azure-ai-voicelive azure-identity python-dotenv pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec7243",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Union, Optional, TYPE_CHECKING, cast, List\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials_async import AsyncTokenCredential\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "from azure.ai.voicelive.aio import connect\n",
    "from azure.ai.voicelive.models import (\n",
    "    AudioEchoCancellation,\n",
    "    AudioInputTranscriptionOptions,\n",
    "    AudioNoiseReduction,\n",
    "    AzureSemanticVad,\n",
    "    AzureStandardVoice,\n",
    "    InputAudioFormat,\n",
    "    Modality,\n",
    "    OutputAudioFormat,\n",
    "    RequestSession,\n",
    "    ServerEventType,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pyaudio\n",
    "\n",
    "# Import reusable AudioProcessor from local module\n",
    "from audio_processor import AudioProcessor\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from azure.ai.voicelive.aio import VoiceLiveConnection\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./.env', override=True)\n",
    "\n",
    "# Setup logging\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.basicConfig(\n",
    "    filename=f'logs/{timestamp}_phrase_list.log',\n",
    "    filemode=\"w\",\n",
    "    format='%(asctime)s:%(name)s:%(levelname)s:%(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"‚úÖ Libraries imported and logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b01fae",
   "metadata": {},
   "source": [
    "## 3. Understanding Phrase Lists\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Purpose** | Boost recognition of specific words/phrases |\n",
    "| **Supported Models** | `azure-speech`, `azure-fast-transcription` |\n",
    "| **NOT Supported** | `whisper-1` (OpenAI Whisper) |\n",
    "| **Max Phrases** | Typically 100-500 phrases recommended |\n",
    "| **Best Practices** | Include variations, common misspellings |\n",
    "\n",
    "### When to Use Phrase Lists\n",
    "\n",
    "‚úÖ **Good Use Cases:**\n",
    "- Product catalogs with unique names\n",
    "- Company-specific terminology\n",
    "- Technical documentation terms\n",
    "- Customer service applications with specific terms\n",
    "\n",
    "‚ùå **Not Recommended:**\n",
    "- Common everyday words\n",
    "- Extremely long phrases (keep to 3-4 words max)\n",
    "- Thousands of phrases (performance impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea14a0e",
   "metadata": {},
   "source": [
    "## 4. Define Phrase List Configuration\n",
    "\n",
    "Here we define different phrase list examples for various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHRASE LIST EXAMPLES FOR DIFFERENT DOMAINS\n",
    "# ============================================================\n",
    "\n",
    "# Example 1: Consumer Electronics / Retail\n",
    "ELECTRONICS_PHRASES: List[str] = [\n",
    "    # TV Products\n",
    "    \"Neo QLED TV\",\n",
    "    \"OLED TV\",\n",
    "    \"Mini LED\",\n",
    "    \"Samsung Frame TV\",\n",
    "    \n",
    "    # Gaming Products\n",
    "    \"TUF Gaming\",\n",
    "    \"ASUS TUF Gaming\",\n",
    "    \"TUF Dash\",\n",
    "    \"ROG Strix\",\n",
    "    \"ROG Zephyrus\",\n",
    "    \"GeForce RTX\",\n",
    "    \"Radeon RX\",\n",
    "    \n",
    "    # Audio Products\n",
    "    \"AirPods Pro\",\n",
    "    \"Galaxy Buds\",\n",
    "    \"WH-1000XM5\",\n",
    "    \"QuietComfort\",\n",
    "]\n",
    "\n",
    "# Example 2: Software / Technology\n",
    "TECH_PHRASES: List[str] = [\n",
    "    # Azure Services\n",
    "    \"Azure OpenAI\",\n",
    "    \"Azure Cosmos DB\",\n",
    "    \"Azure Functions\",\n",
    "    \"Azure AI Foundry\",\n",
    "    \"VoiceLive SDK\",\n",
    "    \n",
    "    # Development Tools\n",
    "    \"VS Code\",\n",
    "    \"GitHub Copilot\",\n",
    "    \"AutoQuote Explorer\",\n",
    "    \"Jupyter Notebook\",\n",
    "    \n",
    "    # Frameworks & Libraries\n",
    "    \"FastAPI\",\n",
    "    \"LangChain\",\n",
    "    \"Semantic Kernel\",\n",
    "    \"PyTorch\",\n",
    "    \"TensorFlow\",\n",
    "]\n",
    "\n",
    "# Example 3: Company/Brand Names\n",
    "BRAND_PHRASES: List[str] = [\n",
    "    \"Microsoft\",\n",
    "    \"OpenAI\",\n",
    "    \"NVIDIA\",\n",
    "    \"AMD\",\n",
    "    \"ASUS\",\n",
    "    \"Advantech\",\n",
    "    \"Contoso\",\n",
    "    \"Fabrikam\",\n",
    "]\n",
    "\n",
    "# Example 4: Chinese Terms (‰∏≠ÊñáÊúØËØ≠)\n",
    "CHINESE_PHRASES: List[str] = [\n",
    "    \"‰∫∫Â∑•Êô∫ËÉΩ\",\n",
    "    \"Êú∫Âô®Â≠¶‰π†\",\n",
    "    \"Ê∑±Â∫¶Â≠¶‰π†\",\n",
    "    \"Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ\",\n",
    "    \"ËØ≠Èü≥ËØÜÂà´\",\n",
    "    \"ËØ≠Èü≥ÂêàÊàê\",\n",
    "    \"Â§ßËØ≠Ë®ÄÊ®°Âûã\",\n",
    "    \"GPT-4o\",\n",
    "]\n",
    "\n",
    "print(\"üìù Phrase list examples defined:\")\n",
    "print(f\"   Electronics: {len(ELECTRONICS_PHRASES)} phrases\")\n",
    "print(f\"   Technology: {len(TECH_PHRASES)} phrases\")\n",
    "print(f\"   Brands: {len(BRAND_PHRASES)} phrases\")\n",
    "print(f\"   Chinese: {len(CHINESE_PHRASES)} phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43633f",
   "metadata": {},
   "source": [
    "## 5. Voice Assistant with Phrase List Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseListVoiceAssistant:\n",
    "    \"\"\"Voice assistant with phrase list support for improved recognition.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        credential: Union[AzureKeyCredential, AsyncTokenCredential],\n",
    "        model: str,\n",
    "        voice: str,\n",
    "        instructions: str,\n",
    "        # Phrase list configuration\n",
    "        phrase_list: List[str],\n",
    "        transcription_model: str = \"azure-speech\",\n",
    "        transcription_language: Optional[str] = None,\n",
    "    ):\n",
    "        self.endpoint = endpoint\n",
    "        self.credential = credential\n",
    "        self.model = model\n",
    "        self.voice = voice\n",
    "        self.instructions = instructions\n",
    "        self.phrase_list = phrase_list\n",
    "        self.transcription_model = transcription_model\n",
    "        \n",
    "        # Auto-detect language from voice if not specified\n",
    "        if transcription_language:\n",
    "            self.transcription_language = transcription_language\n",
    "        elif voice and \"-\" in voice:\n",
    "            parts = voice.split(\"-\")\n",
    "            if len(parts) >= 2:\n",
    "                self.transcription_language = f\"{parts[0]}-{parts[1].split(':')[0]}\"\n",
    "            else:\n",
    "                self.transcription_language = \"en-US\"\n",
    "        else:\n",
    "            self.transcription_language = \"en-US\"\n",
    "            \n",
    "        self.connection: Optional[\"VoiceLiveConnection\"] = None\n",
    "        self.audio_processor: Optional[AudioProcessor] = None\n",
    "        self._active_response = False\n",
    "        self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the voice assistant session.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Connecting to VoiceLive API with phrase list support\")\n",
    "\n",
    "            async with connect(\n",
    "                endpoint=self.endpoint,\n",
    "                credential=self.credential,\n",
    "                model=self.model,\n",
    "            ) as connection:\n",
    "                self.connection = connection\n",
    "                self.audio_processor = AudioProcessor(connection)\n",
    "\n",
    "                await self._setup_session()\n",
    "                self.audio_processor.start_playback()\n",
    "\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"üé§ PHRASE LIST VOICE ASSISTANT READY\")\n",
    "                print(f\"üìù Phrase list: {len(self.phrase_list)} terms configured\")\n",
    "                print(f\"üåê Language: {self.transcription_language}\")\n",
    "                print(f\"üîä Transcription: {self.transcription_model}\")\n",
    "                print(\"\\nTry saying some of your custom phrases!\")\n",
    "                print(\"Use Cmd+Shift+P ‚Üí 'Notebook: Restart Kernel' to exit\")\n",
    "                print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "                await self._process_events()\n",
    "        finally:\n",
    "            if self.audio_processor:\n",
    "                self.audio_processor.shutdown()\n",
    "\n",
    "    async def _setup_session(self):\n",
    "        \"\"\"Configure session with phrase list.\"\"\"\n",
    "        logger.info(f\"Setting up session with {len(self.phrase_list)} phrases\")\n",
    "\n",
    "        # Configure voice\n",
    "        voice_config = AzureStandardVoice(name=self.voice)\n",
    "\n",
    "        # Configure VAD - AzureSemanticVad is required for azure-speech\n",
    "        turn_detection_config = AzureSemanticVad(\n",
    "            threshold=0.3,\n",
    "            prefix_padding_ms=300,\n",
    "            silence_duration_ms=500,\n",
    "        )\n",
    "\n",
    "        # Configure input transcription WITH phrase list\n",
    "        # This is where the phrase list magic happens!\n",
    "        input_transcription_config = AudioInputTranscriptionOptions(\n",
    "            model=self.transcription_model,\n",
    "            language=self.transcription_language,\n",
    "            phrase_list=self.phrase_list,  # <-- Key configuration!\n",
    "        )\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Phrase list configured: {self.phrase_list[:5]}{'...' if len(self.phrase_list) > 5 else ''}\"\n",
    "        )\n",
    "\n",
    "        session_config = RequestSession(\n",
    "            modalities=[Modality.TEXT, Modality.AUDIO],\n",
    "            instructions=self.instructions,\n",
    "            voice=voice_config,\n",
    "            input_audio_format=InputAudioFormat.PCM16,\n",
    "            output_audio_format=OutputAudioFormat.PCM16,\n",
    "            turn_detection=turn_detection_config,\n",
    "            input_audio_echo_cancellation=AudioEchoCancellation(),\n",
    "            input_audio_noise_reduction=AudioNoiseReduction(\n",
    "                type=\"azure_deep_noise_suppression\"),\n",
    "            input_audio_transcription=input_transcription_config,\n",
    "        )\n",
    "\n",
    "        assert self.connection is not None\n",
    "        await self.connection.session.update(session=session_config)\n",
    "        logger.info(\"Session with phrase list configured\")\n",
    "\n",
    "    async def _process_events(self):\n",
    "        \"\"\"Process events from the VoiceLive connection.\"\"\"\n",
    "        assert self.connection is not None\n",
    "        async for event in self.connection:\n",
    "            await self._handle_event(event)\n",
    "\n",
    "    async def _handle_event(self, event):\n",
    "        \"\"\"Handle different types of events.\"\"\"\n",
    "        ap = self.audio_processor\n",
    "        conn = self.connection\n",
    "        assert ap is not None and conn is not None\n",
    "\n",
    "        if event.type == ServerEventType.SESSION_UPDATED:\n",
    "            logger.info(\"Session ready\")\n",
    "            ap.start_capture()\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:\n",
    "            print(\"üé§ Listening...\")\n",
    "            ap.skip_pending_audio()\n",
    "            if self._active_response:\n",
    "                try:\n",
    "                    await conn.response.cancel()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED:\n",
    "            print(\"ü§î Processing...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_CREATED:\n",
    "            self._active_response = True\n",
    "            self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:\n",
    "            ap.queue_audio(event.delta)\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DONE:\n",
    "            print(\"üé§ Ready for next input...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_DONE:\n",
    "            self._active_response = False\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DELTA:\n",
    "            delta = getattr(event, 'delta', None)\n",
    "            if delta:\n",
    "                self._assistant_transcript_buffer += delta\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DONE:\n",
    "            transcript = getattr(event, 'transcript', None) or self._assistant_transcript_buffer\n",
    "            if transcript:\n",
    "                print(f\"ü§ñ Assistant: {transcript}\")\n",
    "            self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "        # Key event: User speech transcription with phrase list applied\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:\n",
    "            transcript = getattr(event, 'transcript', None)\n",
    "            if transcript:\n",
    "                # Check if any phrase list terms were recognized\n",
    "                matched_phrases = [p for p in self.phrase_list if p.lower() in transcript.lower()]\n",
    "                if matched_phrases:\n",
    "                    print(f\"üìù You said: {transcript}\")\n",
    "                    print(f\"   ‚úÖ Recognized phrases: {matched_phrases}\")\n",
    "                else:\n",
    "                    print(f\"üìù You said: {transcript}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED:\n",
    "            error = getattr(event, 'error', None)\n",
    "            logger.warning(f\"Transcription failed: {error}\")\n",
    "\n",
    "        elif event.type == ServerEventType.ERROR:\n",
    "            msg = event.error.message\n",
    "            if \"no active response\" not in msg.lower():\n",
    "                print(f\"‚ùå Error: {msg}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ PhraseListVoiceAssistant class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18272b3",
   "metadata": {},
   "source": [
    "## 6. Audio System Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_system():\n",
    "    \"\"\"Check if audio input/output devices are available.\"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        input_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxInputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        output_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxOutputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        p.terminate()\n",
    "\n",
    "        if not input_devices:\n",
    "            print(\"‚ùå No audio input devices found.\")\n",
    "            return False\n",
    "        if not output_devices:\n",
    "            print(\"‚ùå No audio output devices found.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ Audio: {len(input_devices)} input, {len(output_devices)} output device(s)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "audio_ok = check_audio_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb117f57",
   "metadata": {},
   "source": [
    "## 7. Configuration & Run\n",
    "\n",
    "Configure your phrase list and run the assistant. The key settings are:\n",
    "\n",
    "1. **ACTIVE_PHRASE_LIST**: Choose which phrase list to use\n",
    "2. **TRANSCRIPTION_MODEL**: Must be `azure-speech` or `azure-fast-transcription`\n",
    "3. **VOICE**: Sets the language for transcription automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553590c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Azure Credentials\n",
    "API_KEY = os.environ.get(\"AZURE_VOICELIVE_API_KEY\")\n",
    "ENDPOINT = os.environ.get(\"AZURE_VOICELIVE_ENDPOINT\", \"https://your-resource.services.ai.azure.com/\")\n",
    "MODEL = \"gpt-realtime\"\n",
    "\n",
    "# Voice Configuration\n",
    "# Change to \"en-US-Ava:DragonHDLatestNeural\" for English\n",
    "VOICE = \"zh-CN-Xiaochen:DragonHDLatestNeural\"\n",
    "\n",
    "# ============================================================\n",
    "# PHRASE LIST CONFIGURATION - The focus of this notebook!\n",
    "# ============================================================\n",
    "\n",
    "# Transcription model (phrase list requires azure-speech or azure-fast-transcription)\n",
    "TRANSCRIPTION_MODEL = \"azure-speech\"  # Options: \"azure-speech\", \"azure-fast-transcription\"\n",
    "\n",
    "# Choose your phrase list or create a custom one\n",
    "# Options: ELECTRONICS_PHRASES, TECH_PHRASES, BRAND_PHRASES, CHINESE_PHRASES\n",
    "# Or combine them:\n",
    "ACTIVE_PHRASE_LIST: List[str] = (\n",
    "    ELECTRONICS_PHRASES + \n",
    "    TECH_PHRASES + \n",
    "    BRAND_PHRASES\n",
    ")\n",
    "\n",
    "# Or define your own custom phrases:\n",
    "# ACTIVE_PHRASE_LIST: List[str] = [\n",
    "#     \"Your Custom Term\",\n",
    "#     \"Another Product Name\",\n",
    "#     \"Technical Jargon\",\n",
    "# ]\n",
    "\n",
    "# System instructions\n",
    "INSTRUCTIONS = \"\"\"\n",
    "You are a helpful voice assistant with expertise in technology products.\n",
    "When users mention specific product names or technical terms, acknowledge them correctly.\n",
    "Keep responses concise and conversational.\n",
    "\"\"\"\n",
    "\n",
    "# Authentication\n",
    "USE_TOKEN_CREDENTIAL = False\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìã PHRASE LIST CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìç Endpoint: {ENDPOINT}\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üéôÔ∏è Voice: {VOICE}\")\n",
    "print(f\"üîä Transcription Model: {TRANSCRIPTION_MODEL}\")\n",
    "print(f\"üîë API Key: {'Set' if API_KEY else 'Not set'}\")\n",
    "print()\n",
    "print(f\"üìù PHRASE LIST ({len(ACTIVE_PHRASE_LIST)} terms):\")\n",
    "print(\"-\" * 50)\n",
    "# Display phrases in columns\n",
    "for i, phrase in enumerate(ACTIVE_PHRASE_LIST[:20]):\n",
    "    print(f\"   ‚Ä¢ {phrase}\")\n",
    "if len(ACTIVE_PHRASE_LIST) > 20:\n",
    "    print(f\"   ... and {len(ACTIVE_PHRASE_LIST) - 20} more\")\n",
    "print()\n",
    "\n",
    "# Validate configuration\n",
    "if TRANSCRIPTION_MODEL not in [\"azure-speech\", \"azure-fast-transcription\"]:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Phrase lists only work with azure-speech or azure-fast-transcription!\")\n",
    "    print(f\"   Current model '{TRANSCRIPTION_MODEL}' does not support phrase lists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b112e50",
   "metadata": {},
   "source": [
    "## 8. Run the Voice Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_phrase_list_assistant():\n",
    "    \"\"\"Run the voice assistant with phrase list.\"\"\"\n",
    "    if not API_KEY and not USE_TOKEN_CREDENTIAL:\n",
    "        print(\"‚ùå Error: No authentication provided\")\n",
    "        print(\"Set AZURE_VOICELIVE_API_KEY in .env file\")\n",
    "        return\n",
    "\n",
    "    credential: Union[AzureKeyCredential, AsyncTokenCredential]\n",
    "    if USE_TOKEN_CREDENTIAL:\n",
    "        credential = AzureCliCredential()\n",
    "        print(\"üîê Using Azure CLI credential\")\n",
    "    else:\n",
    "        credential = AzureKeyCredential(API_KEY)\n",
    "        print(\"üîë Using API key credential\")\n",
    "\n",
    "    assistant = PhraseListVoiceAssistant(\n",
    "        endpoint=ENDPOINT,\n",
    "        credential=credential,\n",
    "        model=MODEL,\n",
    "        voice=VOICE,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        phrase_list=ACTIVE_PHRASE_LIST,\n",
    "        transcription_model=TRANSCRIPTION_MODEL,\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéôÔ∏è  Starting Phrase List Voice Assistant...\")\n",
    "    print(f\"üìù {len(ACTIVE_PHRASE_LIST)} phrases configured for enhanced recognition\")\n",
    "\n",
    "    try:\n",
    "        await assistant.start()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Session ended.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Run the assistant\n",
    "if audio_ok:\n",
    "    try:\n",
    "        await run_phrase_list_assistant()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Goodbye!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please fix audio issues before running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fb097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9025e340",
   "metadata": {},
   "source": [
    "## 9. Tips for Effective Phrase Lists\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Include Variations**\n",
    "   ```python\n",
    "   phrase_list = [\n",
    "       \"TUF Gaming\",\n",
    "       \"TUF\",\n",
    "       \"ASUS TUF\",\n",
    "       \"ASUS TUF Gaming\",\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "2. **Include Common Pronunciations**\n",
    "   ```python\n",
    "   phrase_list = [\n",
    "       \"ASUS\",      # Correct\n",
    "       \"A-sus\",     # Common pronunciation\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Group Related Terms**\n",
    "   ```python\n",
    "   # Product line\n",
    "   ROG_PHRASES = [\"ROG\", \"ROG Strix\", \"ROG Zephyrus\", \"ROG Flow\"]\n",
    "   \n",
    "   # Service names\n",
    "   AZURE_PHRASES = [\"Azure\", \"Azure OpenAI\", \"Azure AI Foundry\"]\n",
    "   ```\n",
    "\n",
    "4. **Keep Phrases Short**\n",
    "   - ‚úÖ Good: \"Neo QLED\" (2 words)\n",
    "   - ‚ö†Ô∏è Okay: \"Samsung Neo QLED TV\" (4 words)\n",
    "   - ‚ùå Bad: \"Samsung 65 inch Neo QLED 8K Smart TV\" (too long)\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "- **Recommended**: 50-200 phrases for optimal performance\n",
    "- **Maximum**: ~500 phrases (may impact latency)\n",
    "- **Update Strategy**: Rotate phrases based on context if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8ce7a",
   "metadata": {},
   "source": [
    "## 10. Comparing Recognition: With vs Without Phrase List\n",
    "\n",
    "| Spoken Phrase | Without Phrase List | With Phrase List |\n",
    "|--------------|---------------------|------------------|\n",
    "| \"TUF Gaming laptop\" | \"tough gaming laptop\" | \"TUF Gaming laptop\" ‚úÖ |\n",
    "| \"Neo QLED TV\" | \"neo Q L E D TV\" | \"Neo QLED TV\" ‚úÖ |\n",
    "| \"Azure Cosmos DB\" | \"azure cosmos D B\" | \"Azure Cosmos DB\" ‚úÖ |\n",
    "| \"ROG Strix\" | \"rog stricks\" | \"ROG Strix\" ‚úÖ |\n",
    "\n",
    "The phrase list significantly improves recognition of specialized vocabulary!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
