{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22355652",
   "metadata": {},
   "source": [
    "# Azure VoiceLive SDK - Basic Voice Assistant\n",
    "\n",
    "This notebook demonstrates how to build a real-time voice assistant using the **Azure AI VoiceLive SDK**. The assistant captures audio from your microphone, sends it to Azure VoiceLive API, and plays back the AI-generated audio response.\n",
    "\n",
    "## Features\n",
    "- Real-time bidirectional audio streaming\n",
    "- Voice Activity Detection (VAD) for natural conversation\n",
    "- Echo cancellation and noise reduction\n",
    "- Support for Azure and OpenAI voices\n",
    "\n",
    "## Prerequisites\n",
    "- Azure AI Foundry resource with VoiceLive enabled\n",
    "- Python 3.9+\n",
    "- PyAudio for audio capture/playback\n",
    "- Microphone and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d69d9",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install portaudio via homebrew: brew install portaudio\n",
    "# Then install Python packages\n",
    "%pip install azure-ai-voicelive azure-identity python-dotenv pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2113a",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9be239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Union, Optional, TYPE_CHECKING, cast, List, Dict\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials_async import AsyncTokenCredential\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "from azure.ai.voicelive.aio import connect\n",
    "from azure.ai.voicelive.models import (\n",
    "    AudioEchoCancellation,\n",
    "    AudioInputTranscriptionOptions,\n",
    "    AudioNoiseReduction,\n",
    "    AzureSemanticVad,\n",
    "    AzureStandardVoice,\n",
    "    EouDetection,\n",
    "    InputAudioFormat,\n",
    "    Modality,\n",
    "    OutputAudioFormat,\n",
    "    RequestSession,\n",
    "    ServerEventType,\n",
    "    ServerVad\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pyaudio\n",
    "\n",
    "# Import reusable AudioProcessor from local module\n",
    "from audio_processor import AudioProcessor\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from azure.ai.voicelive.aio import VoiceLiveConnection\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./.env', override=True)\n",
    "\n",
    "# Setup logging\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.basicConfig(\n",
    "    filename=f'logs/{timestamp}_voicelive.log',\n",
    "    filemode=\"w\",\n",
    "    format='%(asctime)s:%(name)s:%(levelname)s:%(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"‚úÖ Libraries imported and logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36eef0",
   "metadata": {},
   "source": [
    "## 3. AudioProcessor\n",
    "\n",
    "The `AudioProcessor` class is imported from `audio_processor.py`. It handles real-time audio capture and playback using PyAudio.\n",
    "\n",
    "**Features:**\n",
    "- PCM16, 24kHz, mono audio format\n",
    "- Callback-based capture and playback threads\n",
    "- Support for barge-in (skip pending audio)\n",
    "- Proper resource cleanup\n",
    "\n",
    "See `audio_processor.py` for the full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819801d",
   "metadata": {},
   "source": [
    "## 4. BasicVoiceAssistant Class\n",
    "\n",
    "The main voice assistant class that manages the VoiceLive connection and handles events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93673743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicVoiceAssistant:\n",
    "    \"\"\"Basic voice assistant implementing the VoiceLive SDK patterns.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        credential: Union[AzureKeyCredential, AsyncTokenCredential],\n",
    "        model: str,\n",
    "        voice: str,\n",
    "        instructions: str,\n",
    "        voice_temperature: Optional[float] = None,\n",
    "        input_audio_transcription_enabled: bool = False,\n",
    "        input_audio_transcription_model: Optional[str] = None,\n",
    "        phrase_list: Optional[List[str]] = None,\n",
    "        transcription_language: Optional[str] = None,\n",
    "        custom_speech_models: Optional[Dict[str, str]] = None,\n",
    "        # VAD configuration\n",
    "        vad_config: Optional[Dict] = None,\n",
    "    ):\n",
    "        self.endpoint = endpoint\n",
    "        self.credential = credential\n",
    "        self.model = model\n",
    "        self.voice = voice\n",
    "        self.instructions = instructions\n",
    "        self.voice_temperature = voice_temperature\n",
    "        self.input_audio_transcription_enabled = input_audio_transcription_enabled\n",
    "        self.input_audio_transcription_model = input_audio_transcription_model\n",
    "        self.phrase_list = phrase_list or []\n",
    "        self.custom_speech_models = custom_speech_models or {}\n",
    "        self.vad_config = vad_config or {}\n",
    "        # Auto-detect language from voice if not specified\n",
    "        if transcription_language:\n",
    "            self.transcription_language = transcription_language\n",
    "        elif voice and \"-\" in voice:\n",
    "            # Extract locale from voice name (e.g., \"zh-CN-Xiaochen:...\" -> \"zh-CN\")\n",
    "            parts = voice.split(\"-\")\n",
    "            if len(parts) >= 2:\n",
    "                self.transcription_language = f\"{parts[0]}-{parts[1].split(':')[0]}\"\n",
    "            else:\n",
    "                self.transcription_language = \"en-US\"\n",
    "        else:\n",
    "            self.transcription_language = \"en-US\"\n",
    "        self.connection: Optional[\"VoiceLiveConnection\"] = None\n",
    "        self.audio_processor: Optional[AudioProcessor] = None\n",
    "        self.session_ready = False\n",
    "        self._active_response = False\n",
    "        self._response_api_done = False\n",
    "        self._stop_requested = False\n",
    "        # Buffer for streaming assistant transcript\n",
    "        self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "    def _is_azure_transcription_model(self) -> bool:\n",
    "        \"\"\"Check if the transcription model is an Azure model that supports phrase_list.\"\"\"\n",
    "        azure_models = [\"azure-speech\", \"azure-fast-transcription\"]\n",
    "        return self.input_audio_transcription_model in azure_models\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Request graceful stop of the voice assistant.\"\"\"\n",
    "        self._stop_requested = True\n",
    "        print(\"\\nüõë Stop requested, shutting down...\")\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the voice assistant session.\"\"\"\n",
    "        try:\n",
    "            logger.info(\n",
    "                \"Connecting to VoiceLive API with model %s\", self.model)\n",
    "\n",
    "            async with connect(\n",
    "                endpoint=self.endpoint,\n",
    "                credential=self.credential,\n",
    "                model=self.model,\n",
    "            ) as connection:\n",
    "                conn = connection\n",
    "                self.connection = conn\n",
    "\n",
    "                ap = AudioProcessor(conn)\n",
    "                self.audio_processor = ap\n",
    "\n",
    "                await self._setup_session()\n",
    "                ap.start_playback()\n",
    "\n",
    "                logger.info(\"Voice assistant ready! Start speaking...\")\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"üé§ VOICE ASSISTANT READY\")\n",
    "                print(\"Start speaking to begin conversation\")\n",
    "                if self.input_audio_transcription_enabled:\n",
    "                    print(\n",
    "                        f\"üìù Input transcription: {self.input_audio_transcription_model}\")\n",
    "                    print(\n",
    "                        f\"üåê Transcription language: {self.transcription_language}\")\n",
    "                    if self.custom_speech_models and self.transcription_language in self.custom_speech_models:\n",
    "                        print(\"üéØ Custom Speech: enabled\")\n",
    "                print(\"Or use Cmd+Shift+P ‚Üí 'Notebook: Restart Kernel' to exit\")\n",
    "                print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "                await self._process_events()\n",
    "        finally:\n",
    "            if self.audio_processor:\n",
    "                self.audio_processor.shutdown()\n",
    "\n",
    "    def _create_vad_config(self) -> Union[AzureSemanticVad, ServerVad]:\n",
    "        \"\"\"Create VAD configuration based on settings.\"\"\"\n",
    "        vad_type = self.vad_config.get(\"type\", \"server_vad\")\n",
    "        \n",
    "        if vad_type == \"azure_semantic_vad\":\n",
    "            # Build end_of_utterance_detection if configured\n",
    "            eou_config = self.vad_config.get(\"end_of_utterance_detection\")\n",
    "            eou_detection = None\n",
    "            if eou_config:\n",
    "                eou_detection = EouDetection(\n",
    "                    model=eou_config.get(\"model\", \"semantic_detection_v1\"),\n",
    "                )\n",
    "            \n",
    "            vad = AzureSemanticVad(\n",
    "                threshold=self.vad_config.get(\"threshold\", 0.5),\n",
    "                prefix_padding_ms=self.vad_config.get(\"prefix_padding_ms\", 300),\n",
    "                speech_duration_ms=self.vad_config.get(\"speech_duration_ms\", 80),\n",
    "                silence_duration_ms=self.vad_config.get(\"silence_duration_ms\", 500),\n",
    "                remove_filler_words=self.vad_config.get(\"remove_filler_words\", False),\n",
    "                end_of_utterance_detection=eou_detection,\n",
    "            )\n",
    "            logger.info(f\"Using AzureSemanticVad with config: {self.vad_config}\")\n",
    "            return vad\n",
    "        else:\n",
    "            # ServerVad configuration\n",
    "            vad = ServerVad(\n",
    "                threshold=self.vad_config.get(\"threshold\", 0.8),\n",
    "                prefix_padding_ms=self.vad_config.get(\"prefix_padding_ms\", 200),\n",
    "                silence_duration_ms=self.vad_config.get(\"silence_duration_ms\", 1000),\n",
    "            )\n",
    "            logger.info(f\"Using ServerVad with config: {self.vad_config}\")\n",
    "            return vad\n",
    "\n",
    "    async def _setup_session(self):\n",
    "        \"\"\"Configure the VoiceLive session for audio conversation.\"\"\"\n",
    "        logger.info(\"Setting up voice conversation session...\")\n",
    "\n",
    "        # Configure Azure HD voice with optional temperature\n",
    "        voice_config: Union[AzureStandardVoice, str]\n",
    "        if self.voice.startswith(\"en-US-\") or self.voice.startswith(\"zh-CN-\") or \"-\" in self.voice:\n",
    "            voice_config = AzureStandardVoice(\n",
    "                name=self.voice,\n",
    "                temperature=self.voice_temperature\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Using Azure HD voice: {self.voice}, temperature: {self.voice_temperature}\")\n",
    "        else:\n",
    "            voice_config = self.voice\n",
    "\n",
    "        # VAD configuration - use custom config or auto-select based on transcription model\n",
    "        if self.vad_config:\n",
    "            turn_detection_config = self._create_vad_config()\n",
    "        elif self.input_audio_transcription_enabled and self._is_azure_transcription_model():\n",
    "            # AzureSemanticVad is REQUIRED when using azure-speech transcription\n",
    "            turn_detection_config = AzureSemanticVad()\n",
    "            logger.info(\"Using default AzureSemanticVad for Azure transcription\")\n",
    "        else:\n",
    "            # ServerVad for other cases (OpenAI whisper, etc.)\n",
    "            turn_detection_config = ServerVad(\n",
    "                threshold=0.8,\n",
    "                prefix_padding_ms=200,\n",
    "                silence_duration_ms=1000,\n",
    "            )\n",
    "            logger.info(\"Using default ServerVad for turn detection\")\n",
    "\n",
    "        # Configure input audio transcription if enabled\n",
    "        input_transcription_config: Optional[AudioInputTranscriptionOptions] = None\n",
    "        if self.input_audio_transcription_enabled:\n",
    "            # phrase_list and custom_speech are only supported for Azure models\n",
    "            if self._is_azure_transcription_model():\n",
    "                input_transcription_config = AudioInputTranscriptionOptions(\n",
    "                    model=self.input_audio_transcription_model,\n",
    "                    language=self.transcription_language,\n",
    "                    phrase_list=self.phrase_list if self.phrase_list else None,\n",
    "                    # Custom Speech model mapping (language -> endpoint ID)\n",
    "                    custom_speech=self.custom_speech_models if self.custom_speech_models else None,\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"Input audio transcription enabled: model={self.input_audio_transcription_model}, \"\n",
    "                    f\"language={self.transcription_language}, phrase_list={self.phrase_list}, \"\n",
    "                    f\"custom_speech={list(self.custom_speech_models.keys()) if self.custom_speech_models else None}\"\n",
    "                )\n",
    "            else:\n",
    "                # For non-Azure models (whisper-1), don't pass phrase_list, language, or custom_speech\n",
    "                input_transcription_config = AudioInputTranscriptionOptions(\n",
    "                    model=self.input_audio_transcription_model,\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"Input audio transcription enabled: model={self.input_audio_transcription_model} \"\n",
    "                    \"(phrase_list/custom_speech not supported)\"\n",
    "                )\n",
    "\n",
    "        session_config = RequestSession(\n",
    "            modalities=[Modality.TEXT, Modality.AUDIO],\n",
    "            instructions=self.instructions,\n",
    "            voice=voice_config,\n",
    "            input_audio_format=InputAudioFormat.PCM16,\n",
    "            output_audio_format=OutputAudioFormat.PCM16,\n",
    "            turn_detection=turn_detection_config,\n",
    "            input_audio_echo_cancellation=AudioEchoCancellation(),\n",
    "            input_audio_noise_reduction=AudioNoiseReduction(\n",
    "                type=\"azure_deep_noise_suppression\"),\n",
    "            input_audio_transcription=input_transcription_config,\n",
    "        )\n",
    "\n",
    "        conn = self.connection\n",
    "        assert conn is not None, \"Connection must be established before setting up session\"\n",
    "        await conn.session.update(session=session_config)\n",
    "        logger.info(\"Session configuration sent\")\n",
    "\n",
    "    async def _process_events(self):\n",
    "        \"\"\"Process events from the VoiceLive connection.\"\"\"\n",
    "        try:\n",
    "            conn = self.connection\n",
    "            assert conn is not None, \"Connection must be established before processing events\"\n",
    "            async for event in conn:\n",
    "                if self._stop_requested:\n",
    "                    logger.info(\"Stop requested, exiting event loop\")\n",
    "                    break\n",
    "                await self._handle_event(event)\n",
    "        except asyncio.CancelledError:\n",
    "            logger.info(\"Event processing cancelled\")\n",
    "            raise\n",
    "        except Exception:\n",
    "            logger.exception(\"Error processing events\")\n",
    "            raise\n",
    "\n",
    "    async def _handle_event(self, event):\n",
    "        \"\"\"Handle different types of events from VoiceLive.\"\"\"\n",
    "        logger.debug(\"Received event: %s\", event.type)\n",
    "        ap = self.audio_processor\n",
    "        conn = self.connection\n",
    "        assert ap is not None, \"AudioProcessor must be initialized\"\n",
    "        assert conn is not None, \"Connection must be established\"\n",
    "\n",
    "        if event.type == ServerEventType.SESSION_UPDATED:\n",
    "            logger.info(\"Session ready: %s\", event.session.id)\n",
    "            self.session_ready = True\n",
    "            ap.start_capture()\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:\n",
    "            logger.info(\"User started speaking - stopping playback\")\n",
    "            print(\"üé§ Listening...\")\n",
    "            ap.skip_pending_audio()\n",
    "\n",
    "            if self._active_response and not self._response_api_done:\n",
    "                try:\n",
    "                    await conn.response.cancel()\n",
    "                    logger.debug(\n",
    "                        \"Cancelled in-progress response due to barge-in\")\n",
    "                except Exception as e:\n",
    "                    if \"no active response\" in str(e).lower():\n",
    "                        logger.debug(\n",
    "                            \"Cancel ignored - response already completed\")\n",
    "                    else:\n",
    "                        logger.warning(\"Cancel failed: %s\", e)\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED:\n",
    "            logger.info(\"üé§ User stopped speaking\")\n",
    "            print(\"ü§î Processing...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_CREATED:\n",
    "            logger.info(\"ü§ñ Assistant response created\")\n",
    "            self._active_response = True\n",
    "            self._response_api_done = False\n",
    "            self._assistant_transcript_buffer = \"\"  # Reset buffer for new response\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:\n",
    "            logger.debug(\"Received audio delta\")\n",
    "            ap.queue_audio(event.delta)\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DONE:\n",
    "            logger.info(\"ü§ñ Assistant finished speaking\")\n",
    "            print(\"üé§ Ready for next input...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_DONE:\n",
    "            logger.info(\"‚úÖ Response complete\")\n",
    "            self._active_response = False\n",
    "            self._response_api_done = True\n",
    "\n",
    "        # Handle assistant audio transcript (output transcription)\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DELTA:\n",
    "            delta = getattr(event, 'delta', None)\n",
    "            if delta:\n",
    "                self._assistant_transcript_buffer += delta\n",
    "                logger.debug(f\"Assistant transcript delta: {delta}\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_TRANSCRIPT_DONE:\n",
    "            transcript = getattr(event, 'transcript', None)\n",
    "            if transcript:\n",
    "                logger.info(f\"ü§ñ Assistant said: {transcript}\")\n",
    "                print(f\"ü§ñ Assistant: {transcript}\")\n",
    "            elif self._assistant_transcript_buffer:\n",
    "                # Use buffered transcript if final event doesn't include it\n",
    "                logger.info(\n",
    "                    f\"ü§ñ Assistant said: {self._assistant_transcript_buffer}\")\n",
    "                print(f\"ü§ñ Assistant: {self._assistant_transcript_buffer}\")\n",
    "            self._assistant_transcript_buffer = \"\"\n",
    "\n",
    "        # Handle input audio transcription events (user speech)\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:\n",
    "            transcript = getattr(event, 'transcript', None)\n",
    "            if transcript:\n",
    "                logger.info(f\"üìù User said: {transcript}\")\n",
    "                print(f\"üìù You said: {transcript}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_DELTA:\n",
    "            delta = getattr(event, 'delta', None)\n",
    "            if delta:\n",
    "                logger.debug(f\"Transcription delta: {delta}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED:\n",
    "            error = getattr(event, 'error', None)\n",
    "            logger.warning(f\"Input transcription failed: {error}\")\n",
    "\n",
    "        elif event.type == ServerEventType.ERROR:\n",
    "            msg = event.error.message\n",
    "            if \"Cancellation failed: no active response\" in msg:\n",
    "                logger.debug(\"Benign cancellation error: %s\", msg)\n",
    "            else:\n",
    "                logger.error(\"‚ùå VoiceLive error: %s\", msg)\n",
    "                print(f\"Error: {msg}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_CREATED:\n",
    "            logger.debug(\"Conversation item created: %s\", event.item.id)\n",
    "\n",
    "        else:\n",
    "            logger.debug(\"Unhandled event type: %s\", event.type)\n",
    "\n",
    "\n",
    "print(\"‚úÖ BasicVoiceAssistant class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44435042",
   "metadata": {},
   "source": [
    "## 5. Check Audio System\n",
    "\n",
    "Verify that audio input/output devices are available before starting the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_system():\n",
    "    \"\"\"Check if audio input/output devices are available.\"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        # Check for input devices\n",
    "        input_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxInputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        # Check for output devices\n",
    "        output_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxOutputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        p.terminate()\n",
    "\n",
    "        if not input_devices:\n",
    "            print(\"‚ùå No audio input devices found. Please check your microphone.\")\n",
    "            return False\n",
    "        if not output_devices:\n",
    "            print(\"‚ùå No audio output devices found. Please check your speakers.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(input_devices)} input device(s) and {len(output_devices)} output device(s)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio system check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the audio check\n",
    "audio_ok = check_audio_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cd337",
   "metadata": {},
   "source": [
    "## 6. Run the Voice Assistant\n",
    "\n",
    "The `run_voice_assistant()` function creates and starts the voice assistant with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_voice_assistant():\n",
    "    \"\"\"Run the voice assistant.\"\"\"\n",
    "    # Validate credentials\n",
    "    if not API_KEY and not USE_TOKEN_CREDENTIAL:\n",
    "        print(\"‚ùå Error: No authentication provided\")\n",
    "        print(\"Please set AZURE_VOICELIVE_API_KEY in the .env file,\")\n",
    "        print(\"or set USE_TOKEN_CREDENTIAL = True for Azure authentication.\")\n",
    "        return\n",
    "\n",
    "    # Create client with appropriate credential\n",
    "    credential: Union[AzureKeyCredential, AsyncTokenCredential]\n",
    "    if USE_TOKEN_CREDENTIAL:\n",
    "        credential = AzureCliCredential()\n",
    "        logger.info(\"Using Azure token credential\")\n",
    "        print(\"üîê Using Azure CLI credential\")\n",
    "    else:\n",
    "        credential = AzureKeyCredential(API_KEY)\n",
    "        logger.info(\"Using API key credential\")\n",
    "        print(\"üîë Using API key credential\")\n",
    "\n",
    "    # Create and start voice assistant\n",
    "    assistant = BasicVoiceAssistant(\n",
    "        endpoint=ENDPOINT,\n",
    "        credential=credential,\n",
    "        model=MODEL,\n",
    "        voice=VOICE,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        voice_temperature=VOICE_TEMPERATURE,\n",
    "        # Input audio transcription settings\n",
    "        input_audio_transcription_enabled=INPUT_AUDIO_TRANSCRIPTION_ENABLED,\n",
    "        input_audio_transcription_model=INPUT_AUDIO_TRANSCRIPTION_MODEL,\n",
    "        phrase_list=PHRASE_LIST,\n",
    "        # Custom Speech model mapping\n",
    "        custom_speech_models=CUSTOM_SPEECH_MODELS if CUSTOM_SPEECH_ENABLED else None,\n",
    "        # VAD configuration\n",
    "        vad_config=VAD_CONFIG,\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéôÔ∏è  Basic Voice Assistant with Azure VoiceLive SDK\")\n",
    "    print(\"=\" * 50)\n",
    "    if INPUT_AUDIO_TRANSCRIPTION_ENABLED:\n",
    "        print(f\"üìù Input transcription: {INPUT_AUDIO_TRANSCRIPTION_MODEL}\")\n",
    "        if CUSTOM_SPEECH_ENABLED and CUSTOM_SPEECH_MODELS:\n",
    "            print(f\"üéØ Custom Speech: enabled for {list(CUSTOM_SPEECH_MODELS.keys())}\")\n",
    "        elif PHRASE_LIST:\n",
    "            print(f\"   Custom phrases: {', '.join(PHRASE_LIST[:3])}{'...' if len(PHRASE_LIST) > 3 else ''}\")\n",
    "    print(f\"üéõÔ∏è VAD: {VAD_CONFIG.get('type', 'auto')}\")\n",
    "\n",
    "    try:\n",
    "        await assistant.start()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Voice assistant shut down. Goodbye!\")\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Voice assistant interrupted. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ run_voice_assistant() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a961c",
   "metadata": {},
   "source": [
    "## 7. Configuration & Run\n",
    "\n",
    "Update the settings below and run this cell to start the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Edit these settings and run this cell\n",
    "# ============================================================\n",
    "\n",
    "# Azure Credentials (loaded from .env file)\n",
    "API_KEY = os.environ.get(\"AZURE_VOICELIVE_API_KEY\")\n",
    "ENDPOINT = os.environ.get(\"AZURE_VOICELIVE_ENDPOINT\",\n",
    "                          \"https://your-resource.services.ai.azure.com/\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Azure HD Voice Configuration\n",
    "# Format: \"{locale}-{VoiceName}:DragonHDLatestNeural\"\n",
    "# Examples:\n",
    "#   - \"en-US-Ava:DragonHDLatestNeural\" (English US)\n",
    "#   - \"zh-CN-Xiaochen:DragonHDLatestNeural\" (Chinese)\n",
    "#   - \"ja-JP-Nanami:DragonHDLatestNeural\" (Japanese)\n",
    "VOICE = \"zh-CN-Xiaochen:DragonHDLatestNeural\"\n",
    "VOICE_TEMPERATURE = 0.8  # Controls voice expressiveness (0.0-1.0)\n",
    "\n",
    "# Input Audio Transcription Configuration\n",
    "INPUT_AUDIO_TRANSCRIPTION_ENABLED = True\n",
    "\n",
    "# Transcription model options:\n",
    "#   - \"whisper-1\": OpenAI Whisper (auto-detect language, no custom speech)\n",
    "#   - \"azure-speech\": Azure Speech (supports phrase_list & custom_speech)\n",
    "#   - \"azure-fast-transcription\": Azure Fast Transcription (supports phrase_list & custom_speech)\n",
    "INPUT_AUDIO_TRANSCRIPTION_MODEL = \"azure-speech\"\n",
    "\n",
    "# Custom phrase list for better recognition of specific terms\n",
    "# Only works with azure-speech and azure-fast-transcription\n",
    "PHRASE_LIST: List[str] = [\n",
    "    \"Neo QLED TV\",\n",
    "    \"TUF Gaming\", \n",
    "    \"TUF\",\n",
    "    \"ASUS TUF\",\n",
    "    \"ASUS TUF Gaming\",\n",
    "    \"AutoQuote Explorer\",\n",
    "    \"asus tuf dash\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# CUSTOM SPEECH CONFIGURATION\n",
    "# ============================================================\n",
    "# Enable Custom Speech for improved domain-specific recognition\n",
    "# Requires azure-speech or azure-fast-transcription model\n",
    "CUSTOM_SPEECH_ENABLED = True\n",
    "\n",
    "# Custom Speech Model Endpoint IDs\n",
    "# Map language codes to your deployed Custom Speech model IDs\n",
    "# Models must be deployed in the SAME region as your Foundry resource\n",
    "# Get the Endpoint ID from your Custom Speech deployment in Azure Speech Studio\n",
    "CUSTOM_SPEECH_MODELS: Dict[str, str] = {\n",
    "    # Chinese Custom Speech model\n",
    "    \"zh-CN\": \"dbdc0514-efcf-49c9-b022-040e463c4725\",\n",
    "    \n",
    "    # Add more language-specific models as needed:\n",
    "    # \"en-US\": \"your-english-model-endpoint-id\",\n",
    "    # \"ja-JP\": \"your-japanese-model-endpoint-id\",\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# VAD (Voice Activity Detection) CONFIGURATION\n",
    "# ============================================================\n",
    "# Two VAD types available:\n",
    "#   - \"azure_semantic_vad\": Advanced semantic-based detection (required for azure-speech)\n",
    "#   - \"server_vad\": Basic threshold-based detection (for whisper-1 or when no transcription)\n",
    "\n",
    "VAD_CONFIG: Dict = {\n",
    "    \"type\": \"azure_semantic_vad\",  # or \"server_vad\"\n",
    "    \n",
    "    # Common settings for both VAD types:\n",
    "    \"threshold\": 0.3,              # Speech detection sensitivity (0.0-1.0, lower = more sensitive)\n",
    "    \"prefix_padding_ms\": 300,      # Audio to include before speech starts (ms)\n",
    "    \"silence_duration_ms\": 500,    # How long to wait after silence to end turn (ms)\n",
    "    \n",
    "    # AzureSemanticVad specific settings:\n",
    "    \"speech_duration_ms\": 80,      # Minimum speech duration to trigger detection (ms)\n",
    "    \"remove_filler_words\": False,  # Remove \"um\", \"uh\", etc. from transcription\n",
    "    \n",
    "    # End-of-utterance detection (AzureSemanticVad only)\n",
    "    # NOTE: EOU detection is only supported for cascaded pipelines (phi4-mm-realtime, etc.)\n",
    "    # Set to None to disable, or provide config for supported models\n",
    "    \"end_of_utterance_detection\": None,  # Disabled - not supported with gpt-realtime\n",
    "    # Example for supported models:\n",
    "    # \"end_of_utterance_detection\": {\n",
    "    #     \"model\": \"semantic_detection_v1\",\n",
    "    # },\n",
    "}\n",
    "\n",
    "# Alternative: ServerVad config (simpler, for non-Azure transcription)\n",
    "# VAD_CONFIG: Dict = {\n",
    "#     \"type\": \"server_vad\",\n",
    "#     \"threshold\": 0.8,              # Higher threshold to reduce false positives\n",
    "#     \"prefix_padding_ms\": 200,\n",
    "#     \"silence_duration_ms\": 1000,   # Wait 1 second of silence\n",
    "# }\n",
    "\n",
    "# System instructions for the AI assistant\n",
    "INSTRUCTIONS = os.environ.get(\n",
    "    \"AZURE_VOICELIVE_INSTRUCTIONS\",\n",
    "    \"You are a helpful AI assistant. Respond naturally and conversationally. Keep your responses concise but engaging.\"\n",
    ")\n",
    "\n",
    "# Authentication method\n",
    "USE_TOKEN_CREDENTIAL = False  # Set True to use Azure CLI credential\n",
    "\n",
    "# ============================================================\n",
    "# AUTO-DETECTION & STATUS\n",
    "# ============================================================\n",
    "\n",
    "def _get_transcription_language(voice: str) -> str:\n",
    "    if voice and \"-\" in voice:\n",
    "        parts = voice.split(\"-\")\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}-{parts[1].split(':')[0]}\"\n",
    "    return \"en-US\"\n",
    "\n",
    "TRANSCRIPTION_LANGUAGE = _get_transcription_language(VOICE)\n",
    "\n",
    "print(f\"üìç Endpoint: {ENDPOINT}\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üéôÔ∏è Voice: {VOICE}\")\n",
    "print(f\"üå°Ô∏è Voice Temperature: {VOICE_TEMPERATURE}\")\n",
    "print(f\"üåê Transcription Language: {TRANSCRIPTION_LANGUAGE} (auto-detected)\")\n",
    "print(f\"üîë API Key: {'Set' if API_KEY else 'Not set'}\")\n",
    "print(f\"üìù Input Transcription: {'Enabled' if INPUT_AUDIO_TRANSCRIPTION_ENABLED else 'Disabled'}\")\n",
    "if INPUT_AUDIO_TRANSCRIPTION_ENABLED:\n",
    "    print(f\"   Model: {INPUT_AUDIO_TRANSCRIPTION_MODEL}\")\n",
    "    if INPUT_AUDIO_TRANSCRIPTION_MODEL in [\"azure-speech\", \"azure-fast-transcription\"]:\n",
    "        print(f\"   Phrase List: {len(PHRASE_LIST)} terms\")\n",
    "        print(f\"üéØ Custom Speech: {'Enabled' if CUSTOM_SPEECH_ENABLED else 'Disabled'}\")\n",
    "        if CUSTOM_SPEECH_ENABLED and CUSTOM_SPEECH_MODELS:\n",
    "            for lang, model_id in CUSTOM_SPEECH_MODELS.items():\n",
    "                print(f\"      {lang}: {model_id[:30]}...\")\n",
    "                \n",
    "print(f\"üéõÔ∏è VAD Type: {VAD_CONFIG.get('type', 'auto')}\")\n",
    "if VAD_CONFIG:\n",
    "    print(f\"   Threshold: {VAD_CONFIG.get('threshold', 'default')}\")\n",
    "    print(f\"   Silence Duration: {VAD_CONFIG.get('silence_duration_ms', 'default')}ms\")\n",
    "    if VAD_CONFIG.get('type') == 'azure_semantic_vad':\n",
    "        print(f\"   Remove Filler Words: {VAD_CONFIG.get('remove_filler_words', False)}\")\n",
    "        eou = VAD_CONFIG.get('end_of_utterance_detection')\n",
    "        print(f\"   EOU Detection: {eou.get('model') if eou else 'Disabled'}\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN THE VOICE ASSISTANT\n",
    "# ============================================================\n",
    "\n",
    "if audio_ok:\n",
    "    try:\n",
    "        await run_voice_assistant()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Session ended. Goodbye!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please fix audio issues before running the voice assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303595f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
