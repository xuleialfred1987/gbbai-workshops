{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb897f0",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install portaudio via homebrew: brew install portaudio\n",
    "# Then install Python packages\n",
    "%pip install azure-ai-voicelive azure-identity python-dotenv pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426ded2",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Union, Optional, TYPE_CHECKING, cast, List, Dict\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials_async import AsyncTokenCredential\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "from azure.ai.voicelive.aio import connect\n",
    "from azure.ai.voicelive.models import (\n",
    "    AudioEchoCancellation,\n",
    "    AudioInputTranscriptionOptions,\n",
    "    AudioNoiseReduction,\n",
    "    AzureSemanticVad,\n",
    "    AzureStandardVoice,\n",
    "    InputAudioFormat,\n",
    "    Modality,\n",
    "    OutputAudioFormat,\n",
    "    RequestSession,\n",
    "    ServerEventType,\n",
    "    ServerVad\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pyaudio\n",
    "\n",
    "# Import reusable AudioProcessor from local module\n",
    "from audio_processor import AudioProcessor\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from azure.ai.voicelive.aio import VoiceLiveConnection\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./.env', override=True)\n",
    "\n",
    "# Setup logging\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.basicConfig(\n",
    "    filename=f'logs/{timestamp}_custom_speech.log',\n",
    "    filemode=\"w\",\n",
    "    format='%(asctime)s:%(name)s:%(levelname)s:%(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"‚úÖ Libraries imported and logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450cb06",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "### Custom Speech Model Configuration\n",
    "\n",
    "The `custom_speech` field maps language codes to Custom Speech model IDs:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input_audio_transcription\": {\n",
    "    \"model\": \"azure-speech\",\n",
    "    \"language\": \"zh-CN\",\n",
    "    \"custom_speech\": {\n",
    "      \"zh-CN\": \"your-custom-model-endpoint-id\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Important Notes:**\n",
    "- Custom Speech model must be deployed in the **same region** as your Foundry resource\n",
    "- The model ID is the Endpoint ID from your Custom Speech deployment\n",
    "- You can specify different models for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4159cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Azure VoiceLive Configuration\n",
    "# ============================================================\n",
    "API_KEY = os.environ.get(\"AZURE_VOICELIVE_API_KEY\")\n",
    "ENDPOINT = os.environ.get(\"AZURE_VOICELIVE_ENDPOINT\",\n",
    "                          \"https://your-resource.services.ai.azure.com/\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Azure HD Voice Configuration\n",
    "VOICE = \"zh-CN-Xiaochen:DragonHDLatestNeural\"  # Chinese voice for this demo\n",
    "VOICE_TEMPERATURE = 0.8\n",
    "\n",
    "# ============================================================\n",
    "# Custom Speech Model Configuration\n",
    "# ============================================================\n",
    "# Enable custom speech for improved domain-specific recognition\n",
    "CUSTOM_SPEECH_ENABLED = True\n",
    "\n",
    "# Azure Speech transcription model (required for custom speech)\n",
    "INPUT_AUDIO_TRANSCRIPTION_MODEL = \"azure-speech\"\n",
    "\n",
    "# Language for transcription (must match your custom speech model)\n",
    "# Options: \"zh-CN\", \"en-US\", \"ja-JP\", etc.\n",
    "TRANSCRIPTION_LANGUAGE = \"zh-CN\"\n",
    "\n",
    "# Custom Speech Model Endpoint IDs\n",
    "# Map language codes to your deployed Custom Speech model IDs\n",
    "# These models must be deployed in the same region as your Foundry resource\n",
    "CUSTOM_SPEECH_MODELS: Dict[str, str] = {\n",
    "    # Your Custom Speech model for Chinese\n",
    "    \"zh-CN\": \"dbdc0514-efcf-49c9-b022-040e463c4725\",\n",
    "    # Add more language-specific models as needed:\n",
    "    # \"en-US\": \"your-english-model-endpoint-id\",\n",
    "    # \"ja-JP\": \"your-japanese-model-endpoint-id\",\n",
    "}\n",
    "\n",
    "# Optional: Phrase list for additional recognition hints\n",
    "# Works alongside custom speech for extra boost\n",
    "PHRASE_LIST: List[str] = [\n",
    "    # Add domain-specific terms here\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# System Instructions\n",
    "# ============================================================\n",
    "INSTRUCTIONS = os.environ.get(\n",
    "    \"AZURE_VOICELIVE_INSTRUCTIONS\",\n",
    "    \"\"\"‰Ω†ÊòØ‰∏Ä‰∏™ÊúâÂ∏ÆÂä©ÁöÑAIÂä©Êâã„ÄÇËØ∑Áî®‰∏≠ÊñáËá™ÁÑ∂Âú∞ÂõûÂ∫îÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇ\n",
    "‰øùÊåÅÂõûÁ≠îÁÆÄÊ¥Å‰ΩÜÊúâÂê∏ÂºïÂäõ„ÄÇ\n",
    "You are a helpful AI assistant. Respond naturally in Chinese.\n",
    "Keep your responses concise but engaging.\"\"\"\n",
    ")\n",
    "\n",
    "# Authentication mode\n",
    "USE_TOKEN_CREDENTIAL = False\n",
    "\n",
    "# ============================================================\n",
    "# Print Configuration Summary\n",
    "# ============================================================\n",
    "print(\"üìã Configuration Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìç Endpoint: {ENDPOINT}\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üéôÔ∏è Voice: {VOICE}\")\n",
    "print(f\"üå°Ô∏è Voice Temperature: {VOICE_TEMPERATURE}\")\n",
    "print(f\"üîë API Key: {'Set' if API_KEY else 'Not set'}\")\n",
    "print()\n",
    "print(\"üìù Custom Speech Configuration:\")\n",
    "print(f\"   Enabled: {CUSTOM_SPEECH_ENABLED}\")\n",
    "print(f\"   Transcription Model: {INPUT_AUDIO_TRANSCRIPTION_MODEL}\")\n",
    "print(f\"   Language: {TRANSCRIPTION_LANGUAGE}\")\n",
    "if CUSTOM_SPEECH_ENABLED:\n",
    "    print(\"   Custom Models:\")\n",
    "    for lang, model_id in CUSTOM_SPEECH_MODELS.items():\n",
    "        print(f\"      {lang}: {model_id[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abe93d",
   "metadata": {},
   "source": [
    "## 4. CustomSpeechVoiceAssistant Class\n",
    "\n",
    "Extended voice assistant class with Custom Speech model support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeechVoiceAssistant:\n",
    "    \"\"\"Voice assistant with Custom Speech model support for improved transcription.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        credential: Union[AzureKeyCredential, AsyncTokenCredential],\n",
    "        model: str,\n",
    "        voice: str,\n",
    "        instructions: str,\n",
    "        voice_temperature: Optional[float] = None,\n",
    "        # Custom Speech configuration\n",
    "        custom_speech_enabled: bool = False,\n",
    "        transcription_model: str = \"azure-speech\",\n",
    "        transcription_language: str = \"zh-CN\",\n",
    "        custom_speech_models: Optional[Dict[str, str]] = None,\n",
    "        phrase_list: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.endpoint = endpoint\n",
    "        self.credential = credential\n",
    "        self.model = model\n",
    "        self.voice = voice\n",
    "        self.instructions = instructions\n",
    "        self.voice_temperature = voice_temperature\n",
    "\n",
    "        # Custom Speech settings\n",
    "        self.custom_speech_enabled = custom_speech_enabled\n",
    "        self.transcription_model = transcription_model\n",
    "        self.transcription_language = transcription_language\n",
    "        self.custom_speech_models = custom_speech_models or {}\n",
    "        self.phrase_list = phrase_list or []\n",
    "\n",
    "        # Connection state\n",
    "        self.connection: Optional[\"VoiceLiveConnection\"] = None\n",
    "        self.audio_processor: Optional[AudioProcessor] = None\n",
    "        self.session_ready = False\n",
    "        self._active_response = False\n",
    "        self._response_api_done = False\n",
    "        self._stop_requested = False\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Request graceful stop of the voice assistant.\"\"\"\n",
    "        self._stop_requested = True\n",
    "        print(\"\\nüõë Stop requested, shutting down...\")\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Start the voice assistant session.\"\"\"\n",
    "        try:\n",
    "            logger.info(\n",
    "                \"Connecting to VoiceLive API with model %s\", self.model)\n",
    "\n",
    "            async with connect(\n",
    "                endpoint=self.endpoint,\n",
    "                credential=self.credential,\n",
    "                model=self.model,\n",
    "            ) as connection:\n",
    "                conn = connection\n",
    "                self.connection = conn\n",
    "\n",
    "                ap = AudioProcessor(conn)\n",
    "                self.audio_processor = ap\n",
    "\n",
    "                await self._setup_session()\n",
    "                ap.start_playback()\n",
    "\n",
    "                logger.info(\"Voice assistant ready! Start speaking...\")\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"üé§ CUSTOM SPEECH VOICE ASSISTANT READY\")\n",
    "                print(f\"Language: {self.transcription_language}\")\n",
    "                if self.custom_speech_enabled:\n",
    "                    model_id = self.custom_speech_models.get(\n",
    "                        self.transcription_language, \"N/A\")\n",
    "                    print(f\"Custom Model: {model_id[:30]}...\")\n",
    "                print(\"Start speaking to begin conversation\")\n",
    "                print(\"Or use Cmd+Shift+P ‚Üí 'Notebook: Restart Kernel' to exit\")\n",
    "                print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "                await self._process_events()\n",
    "        finally:\n",
    "            if self.audio_processor:\n",
    "                self.audio_processor.shutdown()\n",
    "\n",
    "    async def _setup_session(self):\n",
    "        \"\"\"Configure the VoiceLive session with Custom Speech model.\"\"\"\n",
    "        logger.info(\n",
    "            \"Setting up voice conversation session with Custom Speech...\")\n",
    "\n",
    "        # Configure Azure HD voice with optional temperature\n",
    "        voice_config: Union[AzureStandardVoice, str]\n",
    "        if \"-\" in self.voice:\n",
    "            voice_config = AzureStandardVoice(\n",
    "                name=self.voice,\n",
    "                temperature=self.voice_temperature\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Using Azure HD voice: {self.voice}, temperature: {self.voice_temperature}\")\n",
    "        else:\n",
    "            voice_config = self.voice\n",
    "\n",
    "        # VAD configuration\n",
    "        # turn_detection_config = ServerVad(\n",
    "        #     threshold=0.8,\n",
    "        #     prefix_padding_ms=200,\n",
    "        #     silence_duration_ms=1000\n",
    "        # )\n",
    "\n",
    "        turn_detection_config = AzureSemanticVad(\n",
    "            threshold=0.3,\n",
    "            prefix_padding_ms=300,\n",
    "            speech_duration_ms=80,\n",
    "            silence_duration_ms=500,\n",
    "            remove_filler_words=False,\n",
    "        )\n",
    "\n",
    "        # Configure input audio transcription with Custom Speech\n",
    "        input_transcription_config: Optional[AudioInputTranscriptionOptions] = None\n",
    "\n",
    "        if self.custom_speech_enabled:\n",
    "            # Build the transcription options with custom speech model\n",
    "            input_transcription_config = AudioInputTranscriptionOptions(\n",
    "                model=self.transcription_model,\n",
    "                language=self.transcription_language,\n",
    "                # Custom Speech model mapping\n",
    "                custom_speech=self.custom_speech_models if self.custom_speech_models else None,\n",
    "                # Optional: Add phrase list for additional hints\n",
    "                phrase_list=self.phrase_list if self.phrase_list else None,\n",
    "            )\n",
    "\n",
    "            logger.info(\n",
    "                f\"Custom Speech enabled: model={self.transcription_model}, \"\n",
    "                f\"language={self.transcription_language}, \"\n",
    "                f\"custom_speech_models={list(self.custom_speech_models.keys())}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"üéØ Custom Speech configured for: {self.transcription_language}\")\n",
    "\n",
    "        session_config = RequestSession(\n",
    "            modalities=[Modality.TEXT, Modality.AUDIO],\n",
    "            instructions=self.instructions,\n",
    "            voice=voice_config,\n",
    "            input_audio_format=InputAudioFormat.PCM16,\n",
    "            output_audio_format=OutputAudioFormat.PCM16,\n",
    "            turn_detection=turn_detection_config,\n",
    "            input_audio_echo_cancellation=AudioEchoCancellation(),\n",
    "            input_audio_noise_reduction=AudioNoiseReduction(\n",
    "                type=\"azure_deep_noise_suppression\"),\n",
    "            input_audio_transcription=input_transcription_config,\n",
    "        )\n",
    "\n",
    "        conn = self.connection\n",
    "        assert conn is not None, \"Connection must be established before setting up session\"\n",
    "        await conn.session.update(session=session_config)\n",
    "        logger.info(\"Session configuration sent with Custom Speech\")\n",
    "\n",
    "    async def _process_events(self):\n",
    "        \"\"\"Process events from the VoiceLive connection.\"\"\"\n",
    "        try:\n",
    "            conn = self.connection\n",
    "            assert conn is not None, \"Connection must be established before processing events\"\n",
    "            async for event in conn:\n",
    "                if self._stop_requested:\n",
    "                    logger.info(\"Stop requested, exiting event loop\")\n",
    "                    break\n",
    "                await self._handle_event(event)\n",
    "        except asyncio.CancelledError:\n",
    "            logger.info(\"Event processing cancelled\")\n",
    "            raise\n",
    "        except Exception:\n",
    "            logger.exception(\"Error processing events\")\n",
    "            raise\n",
    "\n",
    "    async def _handle_event(self, event):\n",
    "        \"\"\"Handle different types of events from VoiceLive.\"\"\"\n",
    "        logger.debug(\"Received event: %s\", event.type)\n",
    "        ap = self.audio_processor\n",
    "        conn = self.connection\n",
    "        assert ap is not None, \"AudioProcessor must be initialized\"\n",
    "        assert conn is not None, \"Connection must be established\"\n",
    "\n",
    "        if event.type == ServerEventType.SESSION_UPDATED:\n",
    "            logger.info(\"Session ready: %s\", event.session.id)\n",
    "            self.session_ready = True\n",
    "            ap.start_capture()\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:\n",
    "            logger.info(\"User started speaking - stopping playback\")\n",
    "            print(\"üé§ Listening...\")\n",
    "            ap.skip_pending_audio()\n",
    "\n",
    "            if self._active_response and not self._response_api_done:\n",
    "                try:\n",
    "                    await conn.response.cancel()\n",
    "                    logger.debug(\n",
    "                        \"Cancelled in-progress response due to barge-in\")\n",
    "                except Exception as e:\n",
    "                    if \"no active response\" in str(e).lower():\n",
    "                        logger.debug(\n",
    "                            \"Cancel ignored - response already completed\")\n",
    "                    else:\n",
    "                        logger.warning(\"Cancel failed: %s\", e)\n",
    "\n",
    "        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED:\n",
    "            logger.info(\"üé§ User stopped speaking\")\n",
    "            print(\"ü§î Processing...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_CREATED:\n",
    "            logger.info(\"ü§ñ Assistant response created\")\n",
    "            self._active_response = True\n",
    "            self._response_api_done = False\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:\n",
    "            logger.debug(\"Received audio delta\")\n",
    "            ap.queue_audio(event.delta)\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_AUDIO_DONE:\n",
    "            logger.info(\"ü§ñ Assistant finished speaking\")\n",
    "            print(\"üé§ Ready for next input...\")\n",
    "\n",
    "        elif event.type == ServerEventType.RESPONSE_DONE:\n",
    "            logger.info(\"‚úÖ Response complete\")\n",
    "            self._active_response = False\n",
    "            self._response_api_done = True\n",
    "\n",
    "        # Handle input audio transcription events\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:\n",
    "            transcript = getattr(event, 'transcript', None)\n",
    "            if transcript:\n",
    "                logger.info(f\"üìù User said (Custom Speech): {transcript}\")\n",
    "                print(f\"üìù You said: {transcript}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_DELTA:\n",
    "            delta = getattr(event, 'delta', None)\n",
    "            if delta:\n",
    "                logger.debug(f\"Transcription delta: {delta}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED:\n",
    "            error = getattr(event, 'error', None)\n",
    "            logger.warning(f\"Input transcription failed: {error}\")\n",
    "            print(f\"‚ö†Ô∏è Transcription error: {error}\")\n",
    "\n",
    "        elif event.type == ServerEventType.ERROR:\n",
    "            msg = event.error.message\n",
    "            if \"Cancellation failed: no active response\" in msg:\n",
    "                logger.debug(\"Benign cancellation error: %s\", msg)\n",
    "            else:\n",
    "                logger.error(\"‚ùå VoiceLive error: %s\", msg)\n",
    "                print(f\"Error: {msg}\")\n",
    "\n",
    "        elif event.type == ServerEventType.CONVERSATION_ITEM_CREATED:\n",
    "            logger.debug(\"Conversation item created: %s\", event.item.id)\n",
    "\n",
    "        else:\n",
    "            logger.debug(\"Unhandled event type: %s\", event.type)\n",
    "\n",
    "\n",
    "print(\"‚úÖ CustomSpeechVoiceAssistant class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a671b",
   "metadata": {},
   "source": [
    "## 5. Check Audio System\n",
    "\n",
    "Verify that audio input/output devices are available before starting the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38686ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_system():\n",
    "    \"\"\"Check if audio input/output devices are available.\"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        # Check for input devices\n",
    "        input_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxInputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        # Check for output devices\n",
    "        output_devices = [\n",
    "            i for i in range(p.get_device_count())\n",
    "            if cast(Union[int, float], p.get_device_info_by_index(i).get(\"maxOutputChannels\", 0) or 0) > 0\n",
    "        ]\n",
    "        \n",
    "        p.terminate()\n",
    "\n",
    "        if not input_devices:\n",
    "            print(\"‚ùå No audio input devices found. Please check your microphone.\")\n",
    "            return False\n",
    "        if not output_devices:\n",
    "            print(\"‚ùå No audio output devices found. Please check your speakers.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(input_devices)} input device(s) and {len(output_devices)} output device(s)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio system check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the audio check\n",
    "audio_ok = check_audio_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5e887",
   "metadata": {},
   "source": [
    "## 6. Run the Custom Speech Voice Assistant\n",
    "\n",
    "Start the voice assistant with Custom Speech model for improved transcription accuracy.\n",
    "\n",
    "**Note:** This cell will run continuously until you interrupt it. Use the stop button (‚èπ) in the notebook toolbar to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73eeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_custom_speech_assistant():\n",
    "    \"\"\"Run the voice assistant with Custom Speech model.\"\"\"\n",
    "    # Validate credentials\n",
    "    if not API_KEY and not USE_TOKEN_CREDENTIAL:\n",
    "        print(\"‚ùå Error: No authentication provided\")\n",
    "        print(\"Please set AZURE_VOICELIVE_API_KEY in the .env file,\")\n",
    "        print(\"or set USE_TOKEN_CREDENTIAL = True for Azure authentication.\")\n",
    "        return\n",
    "\n",
    "    # Validate Custom Speech configuration\n",
    "    if CUSTOM_SPEECH_ENABLED:\n",
    "        if not CUSTOM_SPEECH_MODELS:\n",
    "            print(\"‚ö†Ô∏è Warning: Custom Speech enabled but no models configured\")\n",
    "        elif TRANSCRIPTION_LANGUAGE not in CUSTOM_SPEECH_MODELS:\n",
    "            print(f\"‚ö†Ô∏è Warning: No custom model for language '{TRANSCRIPTION_LANGUAGE}'\")\n",
    "            print(f\"   Available: {list(CUSTOM_SPEECH_MODELS.keys())}\")\n",
    "\n",
    "    # Create client with appropriate credential\n",
    "    credential: Union[AzureKeyCredential, AsyncTokenCredential]\n",
    "    if USE_TOKEN_CREDENTIAL:\n",
    "        credential = AzureCliCredential()\n",
    "        logger.info(\"Using Azure token credential\")\n",
    "        print(\"üîê Using Azure CLI credential\")\n",
    "    else:\n",
    "        credential = AzureKeyCredential(API_KEY)\n",
    "        logger.info(\"Using API key credential\")\n",
    "        print(\"üîë Using API key credential\")\n",
    "\n",
    "    # Create and start voice assistant\n",
    "    assistant = CustomSpeechVoiceAssistant(\n",
    "        endpoint=ENDPOINT,\n",
    "        credential=credential,\n",
    "        model=MODEL,\n",
    "        voice=VOICE,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        voice_temperature=VOICE_TEMPERATURE,\n",
    "        # Custom Speech settings\n",
    "        custom_speech_enabled=CUSTOM_SPEECH_ENABLED,\n",
    "        transcription_model=INPUT_AUDIO_TRANSCRIPTION_MODEL,\n",
    "        transcription_language=TRANSCRIPTION_LANGUAGE,\n",
    "        custom_speech_models=CUSTOM_SPEECH_MODELS,\n",
    "        phrase_list=PHRASE_LIST,\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéôÔ∏è  Custom Speech Voice Assistant\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìù Transcription Model: {INPUT_AUDIO_TRANSCRIPTION_MODEL}\")\n",
    "    print(f\"üåê Language: {TRANSCRIPTION_LANGUAGE}\")\n",
    "    if CUSTOM_SPEECH_ENABLED and TRANSCRIPTION_LANGUAGE in CUSTOM_SPEECH_MODELS:\n",
    "        print(f\"üéØ Custom Model ID: {CUSTOM_SPEECH_MODELS[TRANSCRIPTION_LANGUAGE][:30]}...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        await assistant.start()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Voice assistant shut down. Goodbye!\")\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Voice assistant interrupted. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal Error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the assistant\n",
    "if audio_ok:\n",
    "    try:\n",
    "        await run_custom_speech_assistant()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nüëã Session ended. Goodbye!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please fix audio issues before running the voice assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec0576",
   "metadata": {},
   "source": [
    "## 7. Multi-Language Custom Speech Example\n",
    "\n",
    "For scenarios requiring multiple languages with custom models, you can configure like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5662f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-language Custom Speech configuration\n",
    "# Each language can have its own custom-trained model\n",
    "\n",
    "MULTI_LANGUAGE_CUSTOM_SPEECH = {\n",
    "    # Chinese - your deployed custom model\n",
    "    \"zh-CN\": \"dbdc0514-efcf-49c9-b022-040e463c4725\",\n",
    "    \n",
    "    # English - add your English custom model when available\n",
    "    # \"en-US\": \"your-english-custom-model-id\",\n",
    "    \n",
    "    # Japanese - add your Japanese custom model when available\n",
    "    # \"ja-JP\": \"your-japanese-custom-model-id\",\n",
    "}\n",
    "\n",
    "print(\"Multi-language Custom Speech Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for lang, model_id in MULTI_LANGUAGE_CUSTOM_SPEECH.items():\n",
    "    print(f\"  {lang}: {model_id}\")\n",
    "print()\n",
    "print(\"üí° Tips:\")\n",
    "print(\"  - Deploy custom models in the SAME region as your Foundry resource\")\n",
    "print(\"  - Custom Speech training and hosting incur additional costs\")\n",
    "print(\"  - For language codes, see Azure Speech Service documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8e5c9",
   "metadata": {},
   "source": [
    "## 8. Transcription Model Comparison\n",
    "\n",
    "Different models support different transcription options:\n",
    "\n",
    "| Model Type | Supported Transcription Models | Custom Speech Support |\n",
    "|------------|-------------------------------|----------------------|\n",
    "| gpt-4o | `whisper-1`, `gpt-4o-transcribe`, `gpt-4o-mini-transcribe` | ‚ùå No |\n",
    "| gpt-realtime | `whisper-1`, `gpt-4o-transcribe`, `gpt-4o-mini-transcribe` | ‚ùå No |\n",
    "| gpt-realtime-mini | `whisper-1`, `gpt-4o-transcribe`, `gpt-4o-mini-transcribe` | ‚ùå No |\n",
    "| phi4-mm-realtime | `azure-speech` | ‚úÖ Yes |\n",
    "| Non-multimodal models | `azure-speech` | ‚úÖ Yes |\n",
    "\n",
    "**Note:** Custom Speech and Phrase List are only supported with `azure-speech` transcription model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print supported configurations\n",
    "def print_transcription_support():\n",
    "    \"\"\"Print transcription model support matrix.\"\"\"\n",
    "    print(\"\\nüìä Transcription Model Support Matrix\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Model':<25} {'Transcription':<30} {'Custom Speech'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    configs = [\n",
    "        (\"gpt-4o\", \"whisper-1, gpt-4o-transcribe\", \"‚ùå\"),\n",
    "        (\"gpt-realtime\", \"whisper-1, gpt-4o-transcribe\", \"‚ùå\"),\n",
    "        (\"gpt-realtime-mini\", \"whisper-1, gpt-4o-transcribe\", \"‚ùå\"),\n",
    "        (\"phi4-mm-realtime\", \"azure-speech\", \"‚úÖ\"),\n",
    "        (\"Non-multimodal\", \"azure-speech\", \"‚úÖ\"),\n",
    "    ]\n",
    "    \n",
    "    for model, transcription, custom in configs:\n",
    "        print(f\"{model:<25} {transcription:<30} {custom}\")\n",
    "    \n",
    "    print(\"\\nüí° Custom Speech requires:\")\n",
    "    print(\"   1. azure-speech as transcription model\")\n",
    "    print(\"   2. Custom model deployed in same region as Foundry resource\")\n",
    "    print(\"   3. Valid endpoint ID from Custom Speech deployment\")\n",
    "\n",
    "print_transcription_support()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
